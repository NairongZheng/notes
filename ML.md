《机器学习》笔记

# 第2章 模型评估与选择

## 2.3 性能度量

<table>
    <tr align='center'>
        <th rowspan ='2'>真实情况</th>
        <th colspan ='2'>预测结果</th>
    </tr>
    <tr align='center'>
        <th colspan ='1'>正例</th>
        <th colspan ='1'>反例</th>
    </tr>
    <tr  align='center'>
        <td>正例</td>
		<td>TP(真正例)</td>
        <td>FN(假反例)</td>
    </tr>
    <tr  align='center'>
        <td>反例</td>
		<td>FP(假正例)</td>
        <td>TN(真反例)</td>
    </tr>
</table>




- 准确率（Accuracy）：对于给定的测试数据集，分类正确的样本数与总样本数之比
  $$
  \frac{TP+TN}{总样本数}
  $$

- 精确率/查准率（Precision）：预测为正的样本中，又多少是真正的正样本
  $$
  \frac{TP}{TP+FP}
  $$

- 召回率/查全率（Recall）：样本中有多少正例被预测正确了
  $$
  \frac{TP}{TP+FN}
  $$



**查准率和查全率是一对矛盾的度量**，所以要怎么判断一个分类器的好坏呢？见下：

以查准率为纵轴、查全率为横轴作图 ，就得到了查准率 查全率曲线，简称“P-R曲线”，显示该曲线的图称为“PR图”。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220702_机器学习.jpg)

在进行比较时，若一个学习器的P-R 曲线被另一个学习器的曲线完全包住则可断言后者的性能优于前者，如A比C好。

如果两个学习器 P-R 曲线发生了交叉，如A与B，就很难断言。但是非要比个高低的话，比较合理的依据就是PR曲线下面积的大小。它在一定程度上表征了学习器在查准率和查全率上取得相对"双高"的比例。

但是这个面积不太好估算，所以人们设计了一些综合考虑查准率、查全率的性能度量。

- 平衡点（Break-Event Point, BEP）：是“查全率=查准率”时的取值。可以认为A比B好。

但是平衡点还是过于简化，更常用的是F1度量：
$$
F1=\frac {2 \times P \times R}{P+R}=\frac{2 \times TP}{样本总数+TP-TN}
$$
当然还有F1的一般形式Fβ，这里就略了。

这个PR曲线也可以用来计算AP和mAP。

当然还有其他性能度量的方法，这里省略。

补充：

- F1是基于查准率与查全率的调和平均定义的：
  $$
  \frac{1}{F1}=\frac{1}{2} \times(\frac{1}{P}+\frac{1}{R})
  $$

- Fβ则是加权调和平均：
  $$
  \frac{1}{F_β}=\frac{1}{1+β^2}\times(\frac{1}{P}+\frac{β^2}{R})
  $$

- 与算数平均和几何平均相比，调和平均更重视较小值

## 2.5 偏差与方差

- 在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛化错误率；
- 随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率；
- 在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220702_2_泛化误差.jpg)

# 第3章 线性模型

## 3.1 基本形式

线性模型试图学得一个通过属性的线性组合来进行预测的函数，即：
$$
f(x)=w_1x_1+w_2x_2+...+w_dx_d+b
$$
一般用向量形式写成：
$$
f(x)=w^Tx+b
$$
**$w$直观表达了各属性在预测中的重要性**，因此线性模型有很好的可解释性。如西瓜问题：
$$
f_{好瓜}(x)=0.2\cdot x_{色泽}+0.5\cdot x_{根蒂}+0.3\cdot x_{敲声}+1
$$
则意味着可通过综合考虑色泽、根蒂和敲声来判断瓜好不好，其中根蒂最要紧，而敲声比色泽更重要。

许多功能更为强大的非线性模型(nonlinear model) 可在线性模型的基础上通过引入层级结构或高维映射而得。

## 3.2 线性回归

下面就是过程，好好读读这几句话

1. 怎么确定w和b：关键在于怎么衡量$f(x)$和$y$之间的差别
2. 均方误差是回归任务中最常用的性能度量，因此可以试图让均方误差最小化
3. 基于均方误差最小化来进行模型求解的方法称为“最小二乘法”
4. 在线性回归中，最小二乘法就是试图找到一条直线，使所有的样本到直线上的欧氏距离之和最小
5. 求解w和b使$E_{(w,b)}=\sum_{i=1}^m(y_i-wx_i-b)^2$最小化的过程，称为**线性回归模型的最小二乘“参数估计”**

假设我们认为示例所对应的输出是在指数尺度上变化，那就可将输出的对数作为线性模型逼近的目标，即：
$$
lny=w^T+b
$$
这就是“对数线性回归”，他实际上是在试图让$e^{w^T+b}$逼近$y$。上式形式上仍是线性回归，但实质上已是在求取输入空间到输出空间的非线性函数映射。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220702_3_对数线性回归.jpg)

## 3.4 线性判别分析

线性判别分析 (Linear Discriminant Analysis，LDA) 是一种经典的线性学习方法，在二分类问题上最早由Fisher提出，亦称“Fisher判别分析”。

LDA：给定训练样法将样例投影到一条使得同样例的投影点尽可能接近、异类样例投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。LDA二维示意图如下：

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220702_4_LDA二维示意图.jpg)

## 3.6 类别不平衡问题

三类做法：

1. 直接对训练集里的反类样例进行"欠采样" ，即去除一些反例使得正、反例数目接近，然后再进行学习；
2. 是对训练集里的正类样例进行"过来样"，即增加一些正例使得正、反例数目接近，然后再进行学习；
3. 直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，将下式嵌入到其决策过程中，称为"阈值移动"

$$
\frac{y^{'}}{1-y^{'}}=\frac{y}{1-y}\times \frac{m^{-}}{m^{+}}
$$

- 上式是类别不平衡学习的一个基本策略——“再缩放”。（具体见书《机器学习》）

# 第4章 决策树

## 4.1 基本流程

决策树是基于树结构来进行决策的，这恰是人类在面临决策问题时一种很自然的处理机制。

例如，我们要对"这是好瓜吗?"这样的问题进行决策时，通常会进行一系列的判断或"子决策"我们先看"它是什么颜色?"，如果是"青绿色"，则我们再看"它的根蒂是什么形态?"，如果是"蜷缩"，我们再判断"它敲起来是什么声音?"，最后，我们得出最终决策：这是个好瓜。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220702_5_西瓜问题的一棵决策树.jpg)

一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220702_6_决策树算法.jpg)

显然，决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：

1. 当前结点包含的样本全属于同一类别，无需划分
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
3. 当前结点包含的样本集合为空，不能划分

## 4.2 划分选择

决策树学习的关键是第8行，即如何选择最优划分属性一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的"纯度"越来越高。

### 4.2.1 信息增益

（这边可以看[参考2](https://blog.csdn.net/lys_828/article/details/108669442)）

通俗说，就是用当前节点用哪个属性来划分会更快更好地分好类，不能分完跟没分似的不是。这边就会牵扯到熵的概念。**熵值越小（混乱程度越低），决策的效果越好**

在分类之前数据有一个熵值，在采用决策树之后也会有熵值。如果最后分类的$熵1+熵2+...<分类前的熵$，说明比原来有进步。也就是**通过对比熵值（不确定性）减少的程度判断此次决策判断的好坏，不是只看分类后熵值的大小，而是要看决策前后熵值变化的情况！！！**

信息增益：（表示特征X使得类Y的不确定性减少的程度）
$$
gain={划分之前的熵}-{划分之后的熵的和}
$$

### 4.2.2 增益率

使用信息增益会有一个bug，就是假如有一个特征是标号，1.2.3...，按照标号进行划分之后，跟没分一样。就不对了。因为由此特征进行决策判断后的结果均为单个的分支，计算熵值的结果也就为0，这样分类的结果**信息增益是最大**的，说明这个特征是非常有用的，如果还是按照信息增益来进行评判，树模型就势必会按照ID进行根节点的选择，而实际上按照这个方式进行决策判断并不可行。于是就有另外的方法：信息增益率和基尼指数。

（都是划分的选择依据，略）

## 4.3 剪枝处理

剪枝(pruning) 是决策树学习算法对付"过拟合"的主要手段.在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得"太好"了，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可通过主动去掉一些分支来降低过拟合的风险。

决策树剪枝的基本策略有“预剪枝”和“后剪枝”：

- 预剪枝：在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点
- 先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点

如何判断决策树泛化性能是否提升呢？

可以用2.2节的方法（当然，本文没有介绍）。就是一部分数据用作测试集。

**（4.3节可以去看看书，整个决策树的过程讲的很清楚）**

## 4.5 多变量决策树

（这边也是看书比较清楚）

决策树所形成的分类边界有一个明显的特点：轴平行(axis- parallel) ，即它的分类边界由若干个与坐标轴平行的分段组成。这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值。但在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似，如下图所示；此时的决策树会相当复杂，由于要进行大量的属性测试，预测时间开销会很大。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220702_7_决策树分类边界1.jpg)

若能使用斜的划分边界，如下图中红色线段所示，则决策树模型将大为简化。"多变量决策树" (multivariate decision tree) 就是能实现这样的"斜划分"甚至更复杂划分的决策树。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220702_7_决策树分类边界2.jpg)

以实现斜划分的多变量决策树为例，在此类决策树中，非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试；换言之，每个非叶结点是一个形如$\sum_{i=1}^dw_ia_i=t$的线性分类器。

于是，与传统的"单变量决策树" (univariate decision tree) 不同，在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。如下图所示：

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220702_7_决策树分类边界3.jpg)

# 第6章 支持向量机

## 6.1 间隔与支持向量

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220703_1_SVM.jpg)

如上图所示，距离超平面最近的这几个训练样本被称为“支持向量”，两个异类支持向量到超平面的距离之和为：
$$
γ=\frac{2}{||w||}
$$
它被称为“间隔”。欲找到具有"最大间隔" (maximum margin) 的划分超平面。即：
$$
\underset{w,b}{max}=\frac{2}{||w||}  \\
s.t.\ y_i(w^Tx_i+b)\geqslant1,\ i=1,2,...,m
$$

为了最大化间隔，仅需最大化$||w||^{-1}$，这等价于最小化$||w||^2$，于是可以重写为：
$$
\underset{w,b}{max}=\frac{1}{2}||w||^2  \\s.t.\ y_i(w^Tx_i+b)\geqslant1,\ i=1,2,...,m
$$
这就是支持向量机的基本型。

## 6.3 核函数

在本章前面的讨论中，我们假设训练样本是线性可分的，即存在一个划超平面将训练样本正确分类。然而在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面，如下图的“异或”问题就不是线性可分的。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220703_2_异或问题.jpg)

对这样的问题，将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。若将原始的二维映射到一个合适的三维空间，就能找到 一个合适的划分超平面，幸运的是，如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分。

也就是说：

我们观察一下上面这个图，会发现一个问题，就是我们**无论如何也不可能找到一条线把上面这个分类完成**。因为一条线只能分出两个区域，但是上面这个图明显有四个区域。

那如果我们把上面的数据映射到更高的维度当中，上图是二维的图像，我们把它映射到三维当中，就可以使用一个平面将样本区分开了。也就是说**通过一个映射函数，将样本从n维映射到n+1或者更高的维度**，使得原本线性不可分的数据变成线性可分，这样我们就解决了一些原本不能解决的问题。

所以核函数是什么？是一系列函数的统称，这些函数的输入是样本$x$，输出是一个映射到更高维度的样本$x_t$。大部分能实现这一点的函数都可以认为是核函数（不完全准确，只是为了理解方便），当然一些稀奇古怪的函数虽然是核函数，但是对我们的价值可能并不大，所以我们也很少用，常用的核函数还是只有少数几种。

现在我们已经知道核函数是什么了，那么它又该如何使用呢？

用一个字母$Φ$来表示核函数，核函数的输入样本是$x$，所以映射之后的样本就是$Φ(x)$。

但是会带来另一个问题，就是本来低维可能只需要10次计算，但是用核函数映射到高维之后，需要1000次才能得到结果。所以我们对核函数进行了一些限制，就是它的计算复杂度。需要满足的条件（我们把满足条件的核函数成为K）：
$$
K(x_i,x_j)=K(x_i^Tx_j)=Φ(x_i)^TΦ(x_j)
$$
也就是说，$K$对$x_i^Tx_j$的结果进行计算等价于映射之后的结果再进行点乘操作。。。。

常见的核函数：

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220703_3_常见的核函数.jpg)

## 6.4 软间隔与正则化

在前面的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分；退一步说即使恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的。

缓解该问题的一个办法就是**允许支持向量机在一些样本上出错**。为此，要引入“软间隔“的概念（前面所有样本必须正确划分的叫”硬间隔“）：

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220703_4_软间隔.jpg)

## 6.5 支持向量回归

假设要学习一个回归模型$f(x)=w^Tx+b$，传统的回归模型通常直接基于输出$f(x)$与真实输出$y$之间的差别来计算损失，当且仅当二者完全相同时，损失才为零。

与此不同，支持向量回归（Support Vector Regression，SVR）假设我们能容忍$f(x)$与$y$之间最多有$\epsilon$的偏差，且仅当二者之间的差别绝对值大于$\epsilon$时才计算损失。相当于以$f(x)$为中心，构建一个宽度为$2\epsilon$的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。如下图所示：

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220703_5_支持向量回归.jpg)

（很多数学公式推导都没写，可以去看书）

# 第8章集成学习

## 8.1 个体与集成

集成学习的一般结构：先产生一组“个体学习器”(individual learner)，再用某种策略将它们结合起来。

个体学习器通常由一个现有的学习算法从训练数据产生，例如决策树算法、BP神经网络算法等，此时集成中只包含同种类型的个体学习器，例如“决策树集成”中全是决策树，“神经网络集成”中全是神经网络，这样的集成是“同质”的(homogeneous)。同质集成中的个体学习器亦称“基学习器”(base learner)，相应的学习算法称为“基学习算法”(base learning algorithm)。

集成也可包含不同类型的个体学习器，例如同时包含决策树和神经网络，这样的集成是“异质”的(heterogenous)。异质集成中的个体学习器由不同的学习算法生成，这时就不再有基学习算法；相应的，个体学习器一般不称为基学习器，常称为“组件学习器”(component learner)或直接称为个体学习器。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220703_6_集成学习.jpg)

*在一般经验中，如呆把好坏不等的东西掺到一起，那么通常结果会是比最坏的要好一些，比最好的要坏一些。集成学习把多个学习器结合起来，如何能获得比最好的单一学习器更好的性能呢？*

书中举了一个例子说明了：要获得好的集成，个体学习器应”**好而不同**“，即个体学习器要有一定的”准确性“，即**学习器不能太坏**（保证继承不会产生负效果），并且要有”多样性“，即**学习器之间具有差异**（保证集成不会不起作用）。

（中间有些公式分析）

上面的分析有一个关键假设：基学习器的误差相互独立。在现实任务中，个体学习器是为解决同一个问题训练出来的，它们显然不可能相互独立！事实上，个体学习器的"准确性"和"多样性"本身就存在冲突！！一般的，准确性很高之后，要增加多样性就需牺牲准确性。事实上，**如何产生并结合"好而不同"的个体学习器，恰是集成学习研究的核心**。

根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是 Boosting ，后者的代表是 Bagging 和"随机森林" (Random Forest)。

## 8.2 Boosting

Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的T值最终将这T个基学习器进行加权结合。

Boosting 算法要求基学习器能对特定的数据分布进行学习，这可通过"重赋权法" (re-weighting) 实施，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。对无法接受带权样本的基学习算法，则可通过"重采样法" (re-sampling) 来处理，即在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练。

一般而言，这两种做法没有显著的优劣差别。需注意的是， Boosting 算法在训练的每一轮都要检查当前生成的基学习器是否满足基本条件（检查当前基分类器是否是比随机猜测好），一旦条件不满足，则当前基学习器即被抛弃，且学习过程停止。在此种情形下，初始设置的学习轮数T也许远未达到，可能导致最终集成中只包含很少的基学习器而性能不佳。

若采用"重采样法"，则可获得"重启动"机会以避免训练过程过早停止，即在抛弃不满足条件的当前基学习器之后，可根据当前分布重新对训练样本进行采样，再基于新的采样结果重新训练出基学习器，从而使得学习过程可以持续到预设的T轮完成。

下图是一个例子：

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220703_7_boosting西瓜例子.jpg)

## 8.3 Bagging与随机森林

欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然"独立"在现实任务中无法做到，但可以设法使基学习器尽可能具有较大的差异。给定一个训练数据集。一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器。这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异。然而，为获得好的集成，我们同时还希望个体学习器不能太差。如果来样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器。为解决这个问题，我们可考虑**使用相互有交叠的采样子集**。

### 8.3.1 Bagging

Bagging基于2.2.3节介绍的”自主采样法“，具体看书吧~

给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。由式(2.1)可知，初始训练集中约有 63.2%的样本出现在来样集中。

照这样，我们可采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。这就是**Bagging 的基本流程**。

在对预测输出进行结合时， Bagging 通常**对分类任务使用简单投票法，对回归任务使用简单平均法**。若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。

### 8.3.2 随机森林

随机森林(Random Forest，RF)是Bagging的一个扩展变体。RF在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合(假定有d个属性)中选择一个最优属性；

而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度；若令 $k=d$ 基决策树的构建与传统决策树相同；若 令$k=1$ 则是随机选择一个属性用于划分；一般情况下，推荐值$k=log_2d$。

随机森林简单、容易 现、计算开销 小，令人惊奇的是， 它在很多现实任务中展现出强大的性能，被誉为"代表集成学习 技术水平的方法"可以看出，随机森林对 Bagging 只做了小改动，但是与 Bagging中基学习器的"多样性"仅通过样本扰动（通过对初始训练集采样）而来不同，**随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动**，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220703_8_随机森林与bagging对比.jpg)

随机森林的收敛性与 Bagging 相似。如上图，随机森林的起始性能往往相对较差， 特别是在集成中只包含一个基学习器时。这很容易理解，因为通过引入属性扰动，随机森林中个体学习器的性能往往有所降低。然而，随着个体学习器数目的增加 ，随机森林通常会收敛到更低的泛化误差。

值得一提的是，随机森林的训练效率常优于 Bagging，因为在个体决策树的构建过程中 Bagging 使用 的是 确定型" 决策树，在选择划分属性时要对结点的所有属行考察，而随机森林使用的" 随机型"决策树则只需考察属性子集。

# 第9章 聚类

## 9.1 聚类任务

聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个"簇" (cluster)。通过这样的划分，每个簇可能对应于一些潜在的概念(类别) ，如"浅色瓜" "深色瓜"，"有籽瓜" "无籽瓜"，甚至"本地瓜""外地瓜"等；需说明的是，这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名。（聚类是无监督的）

聚类既能作为一个单独过程，用于找寻数据内在的分布结构，也可作为分类等其他学习任务的前驱过程。

例如，在一些商业应用中需对新用户的类型进行判别，但定义"用户类型"对商家来说却可能不太容易，此时往往可先对用户数据进行聚类，根据聚类结果将每个簇定义为一个类，然后再基于这些类训练分类模型，用于判别新用户的类型。

## 9.4原型聚类

### 9.4.1 k均值算法

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220703_9_k均值算法.jpg)

# 第10章 降维与度量学习

## 10.1 k近邻学习

k近邻（k-Nearest Neighbor，kNN） 学习是一种常用的**监督学习**方法，其工作机制非常简单：

给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个"邻居"的信息来进行预测。

在分类任务中可使用"投票法" ，即选择这k个样本中出现最多的类别标记作为预测结果；

在回归任务中时使用"平均法" ，即将k个样本的实值输出标记的平均值作为预测结果；

还可基于距离远近进行加权平均或加权投票，距离越近的样本权重越大。

k近邻学习没有显式的训练过程！事实上，它是"懒惰学习"著名代表，此类学习技术在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后再进行处理；相应的，那些在训练阶段就对样本进行学习处理的方法，称为"急切学习"。

![](https://cdn.jsdelivr.net/gh/Damon-X46/ImgHosting/images/20220703_10_k近邻学习.jpg)





# 参考

1. 周志华. 机器学习[M]. 清华大学出版社, 2016.
2. https://blog.csdn.net/lys_828/article/details/108669442
3. https://zhuanlan.zhihu.com/p/261061617
