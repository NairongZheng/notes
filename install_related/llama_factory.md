
- [ä¸‹è½½ä¸å®‰è£…](#ä¸‹è½½ä¸å®‰è£…)
- [æ¨¡å‹ä¸‹è½½ä¸éªŒè¯](#æ¨¡å‹ä¸‹è½½ä¸éªŒè¯)
- [è‡ªå®šä¹‰æ•°æ®é›†æ„å»º](#è‡ªå®šä¹‰æ•°æ®é›†æ„å»º)
- [åŸºäºLoRAçš„sftæŒ‡ä»¤å¾®è°ƒ](#åŸºäºloraçš„sftæŒ‡ä»¤å¾®è°ƒ)
- [åŠ¨æ€åˆå¹¶LoRAçš„æ¨ç†](#åŠ¨æ€åˆå¹¶loraçš„æ¨ç†)
- [ä¸€ç«™å¼webui boardçš„ä½¿ç”¨](#ä¸€ç«™å¼webui-boardçš„ä½¿ç”¨)
- [å…¶ä»–](#å…¶ä»–)

[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

[å‚è€ƒé“¾æ¥](https://zhuanlan.zhihu.com/p/695287607)

## ä¸‹è½½ä¸å®‰è£…

**ä»æºç å®‰è£…**

```bash
conda create -n dev python=3.10
conda activate dev
cd ~/code
git clone https://github.com/hiyouga/LLaMA-Factory.git
# git clone https://gitee.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e '.[torch,metrics]'
```

**éªŒè¯ç¯å¢ƒ**

```bash
(dev) root@autodl-container-c8da1195fa-d0d8249f:~/code/LLaMA-Factory# python
Python 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> torch.cuda.current_device()
0
>>> torch.cuda.get_device_name(0)
'Tesla V100S-PCIE-32GB'
>>> torch.__version__
'2.7.1+cu126'
>>> quit()
(dev) root@autodl-container-c8da1195fa-d0d8249f:~/code/LLaMA-Factory#
```

## æ¨¡å‹ä¸‹è½½ä¸éªŒè¯

**æ¨¡å‹ä¸‹è½½**

```bash
cd ~/autodl-tmp/model # æ•°æ®ç›˜
# git clone https://www.modelscope.cn/LLM-Research/Meta-Llama-3-8B-Instruct.git
# git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
```

å¥½åƒè¿™ä¹ˆä¸‹è½½ä¼šæœ‰é—®é¢˜ï¼Œå¯ä»¥ç”¨pythonä»£ç ä¸‹è½½ï¼š

```python
# æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct', cache_dir='./cache')
```

ä¸‹è½½å®Œå¤§æ¦‚æ˜¯è¿™æ ·ï¼š

```bash
(dev) root@autodl-container-c8da1195fa-d0d8249f:~/autodl-tmp/model/cache/LLM-Research/Meta-Llama-3-8B-Instruct# du -sh *
8.0K    LICENSE
40K     README.md
8.0K    USE_POLICY.md
4.0K    config.json
4.0K    configuration.json
4.0K    generation_config.json
4.7G    model-00001-of-00004.safetensors
4.7G    model-00002-of-00004.safetensors
4.6G    model-00003-of-00004.safetensors
1.1G    model-00004-of-00004.safetensors
24K     model.safetensors.index.json
15G     original
4.0K    special_tokens_map.json
8.7M    tokenizer.json
52K     tokenizer_config.json
```

**éªŒè¯æ¨¡å‹æ–‡ä»¶çš„æ­£ç¡®æ€§**

```python
import transformers
import torch

# åˆ‡æ¢ä¸ºä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶ç›®å½•, è¿™é‡Œçš„demoæ˜¯Llama-3-8B-Instruct
# å¦‚æœæ˜¯å…¶ä»–æ¨¡å‹ï¼Œæ¯”å¦‚qwenï¼Œchatglmï¼Œè¯·ä½¿ç”¨å…¶å¯¹åº”çš„å®˜æ–¹demo
model_id = "/root/autodl-tmp/model/cache/LLM-Research/Meta-Llama-3-8B-Instruct"

pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)

messages = [
    {"role": "system", "content": "You are a Chinese chatbot who always responds in Chinese speak!"},
    {"role": "user", "content": "Who are you?"},
]

prompt = pipeline.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
)

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = pipeline(
    prompt,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
print(outputs[0]["generated_text"][len(prompt):])
```

ç»“æœå¦‚ä¸‹ï¼š

```bash
(dev) root@autodl-container-c8da1195fa-d0d8249f:~/code# python test_model.py
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.22s/it]
Device set to use cuda:0
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
(nÇ hÇo) WÇ’ jiÃ o zhÅng wÃ©n, shÃ¬ yÄ« gÃ¨ zhÅng guÃ³ de fÄ“ng yÇ” bÇn bÇo (Nice to meet you! My name is Zhong Wen, I'm a Chinese chatbot).
```

**åŸå§‹æ¨¡å‹ç›´æ¥æ¨ç†**

å¯ä»¥ç›´æ¥åœ¨å‘½ä»¤è¡Œï¼š

```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli webchat \
    --model_name_or_path /root/autodl-tmp/model/cache/LLM-Research/Meta-Llama-3-8B-Instruct \
    --template llama3
```

ä¹Ÿå¯ä»¥å°†å…¶ä¸­ä¸€äº›å‚æ•°å†™åˆ°é…ç½®æ–‡ä»¶ä¸­ï¼Œé…ç½®æ–‡ä»¶åœ¨ï¼š`/root/code/LLaMA-Factory/examples/inference/llama3.yaml`

å†…å®¹å¦‚ä¸‹ï¼š

```yaml
model_name_or_path: /root/autodl-tmp/model/cache/LLM-Research/Meta-Llama-3-8B-Instruct
template: llama3
infer_backend: huggingface  # choices: [huggingface, vllm, sglang]
trust_remote_code: true
```

ç„¶åè¿è¡Œ

```bash
llamafactory-cli webchat /root/code/LLaMA-Factory/examples/inference/llama3.yaml
```

ä¼šæœ‰ä»¥ä¸‹logï¼š

```bash
[INFO|2025-06-24 17:25:18] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-24 17:25:18] llamafactory.model.loader:143 >> all params: 8,030,261,248
* Running on local URL:  http://0.0.0.0:7860
* To create a public link, set `share=True` in `launch()`.
```

å› ä¸ºæˆ‘æ˜¯åœ¨è¿œç¨‹æœåŠ¡å™¨è·‘çš„ï¼Œæ‰€ä»¥éœ€è¦åšä¸ªæœ¬åœ°ç«¯å£è½¬å‘æ‰å¯ä»¥åœ¨windowsçš„ç½‘é¡µè®¿é—®ï¼Œç»“æœå¦‚å›¾ï¼š

![llamafactoryçš„web](../images/2025/20250624_llamafactoryçš„web.jpg)


## è‡ªå®šä¹‰æ•°æ®é›†æ„å»º

[æ•°æ®é›†ä»‹ç»](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md)

ç³»ç»Ÿç›®å‰æ”¯æŒ alpaca å’Œsharegptä¸¤ç§æ•°æ®æ ¼å¼ï¼Œä»¥alpacaä¸ºä¾‹ï¼Œæ•´ä¸ªæ•°æ®é›†æ˜¯ä¸€ä¸ªjsonå¯¹è±¡çš„listï¼Œå…·ä½“æ•°æ®æ ¼å¼ä¸ºï¼š

```bash
[
  {
    "instruction": "ç”¨æˆ·æŒ‡ä»¤ï¼ˆå¿…å¡«ï¼‰",
    "input": "ç”¨æˆ·è¾“å…¥ï¼ˆé€‰å¡«ï¼‰",
    "output": "æ¨¡å‹å›ç­”ï¼ˆå¿…å¡«ï¼‰",
    "system": "ç³»ç»Ÿæç¤ºè¯ï¼ˆé€‰å¡«ï¼‰",
    "history": [
      ["ç¬¬ä¸€è½®æŒ‡ä»¤ï¼ˆé€‰å¡«ï¼‰", "ç¬¬ä¸€è½®å›ç­”ï¼ˆé€‰å¡«ï¼‰"],
      ["ç¬¬äºŒè½®æŒ‡ä»¤ï¼ˆé€‰å¡«ï¼‰", "ç¬¬äºŒè½®å›ç­”ï¼ˆé€‰å¡«ï¼‰"]
    ]
  }
]
```

æ¯”å¦‚ï¼š

```bash
{
  "instruction": "è°¢è°¢",
  "input": "",
  "output": "ä¸ç”¨è°¢! å¾ˆé«˜å…´æˆ‘æä¾›çš„ä¿¡æ¯èƒ½å¤Ÿå¸®åŠ©åˆ°ä½ ! å¦‚æœè¿˜æœ‰ä»€ä¹ˆå…¶ä»–é—®é¢˜ä¹Ÿå¯ä»¥å‘æˆ‘æé—®ã€‚",
  "history": [
    [
      "è¯·ä½ ç»™æˆ‘å†™ä¸€ä¸ªé¢è¯•å‡†å¤‡è®¡åˆ’ï¼Œæˆ‘æƒ³è¦å»é¢è¯•å¾®è½¯çš„ç¨‹åºå‘˜å²—ä½",
      "é¦–å…ˆï¼Œä½ å¯ä»¥å»å¾®è½¯å®˜ç½‘å¯»æ‰¾æ‹›è˜ä¿¡æ¯å¹¶ç”³è¯·é¢è¯•ã€‚\nå…¶æ¬¡ï¼Œæ‚¨å¯ä»¥åœ¨ç¤¾äº¤åª’ä½“å¹³å°å¯»æ‰¾å¾®è½¯å…¬å¸å¯¹ç¨‹åºå‘˜çš„é¢è¯•é—®é¢˜ï¼Œå¹¶åšå¥½å‡†å¤‡ã€‚\næœ€åï¼Œæ‚¨å¯ä»¥è‡ªå·±å¯¹é¢è¯•è¿‡ç¨‹è¿›è¡Œæ¨¡æ‹Ÿï¼Œç†Ÿæ‚‰è¯é¢˜å¹¶å‡å°‘ç´§å¼ æ„Ÿã€‚\næˆ‘å¸Œæœ›ä½ èƒ½é¢è¯•æˆåŠŸã€‚"
    ]
  ]
}
```

> æ‰€æœ‰æ•°æ®é›†éƒ½åœ¨`data/dataset_info.json`ä¸­æ³¨å†Œ

**ç³»ç»Ÿè‡ªå¸¦çš„identityæ•°æ®é›†**

```bash
# æ–‡ä»¶è·¯å¾„ï¼šdata/identity.json
# å°†å…¶ä¸­çš„{{name}}è·Ÿ{{author}}è¿›è¡Œæ›¿æ¢
cd ~/code/LLaMA-Factory
sed -i 's/{{name}}/DamonzhengBot/g' data/identity.json
sed -i 's/{{author}}/LLaMA Factory-damonzheng/g' data/identity.json
```

**å•†å“æ–‡æ¡ˆç”Ÿæˆæ•°æ®é›†**

[ä¸‹è½½é“¾æ¥](https://link.zhihu.com/?target=https%3A//cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/%3Fdl%3D1)

åŸå§‹æ ¼å¼å¦‚ä¸‹ï¼Œå¾ˆæ˜æ˜¾ï¼Œè®­ç»ƒç›®æ ‡æ˜¯è¾“å…¥content ï¼ˆä¹Ÿå°±æ˜¯promptï¼‰, è¾“å‡º summary ï¼ˆå¯¹åº”responseï¼‰ï¼š

```bash
{
    "content": "ç±»å‹#è£¤*ç‰ˆå‹#å®½æ¾*é£æ ¼#æ€§æ„Ÿ*å›¾æ¡ˆ#çº¿æ¡*è£¤å‹#é˜”è…¿è£¤", 
    "summary": "å®½æ¾çš„é˜”è…¿è£¤è¿™ä¸¤å¹´çœŸçš„å¸ç²‰ä¸å°‘ï¼Œæ˜æ˜Ÿæ—¶å°šè¾¾äººçš„å¿ƒå¤´çˆ±ã€‚æ¯•ç«Ÿå¥½ç©¿æ—¶å°šï¼Œè°éƒ½èƒ½ç©¿å‡ºè…¿é•¿2ç±³çš„æ•ˆæœå®½æ¾çš„è£¤è…¿ï¼Œå½“ç„¶æ˜¯é®è‚‰å°èƒ½æ‰‹å•Šã€‚ä¸Šèº«éšæ€§è‡ªç„¶ä¸æ‹˜æŸï¼Œé¢æ–™äº²è‚¤èˆ’é€‚è´´èº«ä½“éªŒæ„Ÿæ£’æ£’å“’ã€‚ç³»å¸¦éƒ¨åˆ†å¢åŠ è®¾è®¡çœ‹ç‚¹ï¼Œè¿˜è®©å•å“çš„è®¾è®¡æ„Ÿæ›´å¼ºã€‚è…¿éƒ¨çº¿æ¡è‹¥éšè‹¥ç°çš„ï¼Œæ€§æ„Ÿæ’©äººã€‚é¢œè‰²æ•²æ¸©æŸ”çš„ï¼Œä¸è£¤å­æœ¬èº«æ‰€å‘ˆç°çš„é£æ ¼æœ‰ç‚¹åå·®èŒã€‚"
}
```

éœ€è¦ï¼š
- å¤åˆ¶è¯¥æ•°æ®é›†åˆ°dataç›®å½•ä¸‹
- ä¿®æ”¹ `data/dataset_info.json` æ–°åŠ å†…å®¹å®Œæˆæ³¨å†Œï¼Œæ³¨å†Œå†…å®¹å¦‚ä¸‹ï¼š

```bash
"adgen_local": {
    "file_name": "AdvertiseGen/train.json",
    "columns": {
        "prompt": "content",
        "response": "summary"
    }
}
```

## åŸºäºLoRAçš„sftæŒ‡ä»¤å¾®è°ƒ

åœ¨å‡†å¤‡å¥½æ•°æ®é›†ä¹‹åï¼Œå°±å¯ä»¥å¼€å§‹å‡†å¤‡è®­ç»ƒäº†ï¼Œæˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯è®©åŸæ¥çš„LLaMA3æ¨¡å‹èƒ½å¤Ÿå­¦ä¼šæˆ‘ä»¬å®šä¹‰çš„â€œä½ æ˜¯è°â€ï¼ŒåŒæ—¶å­¦ä¼šå•†å“æ–‡æ¡ˆçš„ä¸€äº›ç”Ÿæˆã€‚

```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \
    --stage sft \
    --do_train \
    --model_name_or_path /root/autodl-tmp/model/cache/LLM-Research/Meta-Llama-3-8B-Instruct \
    --dataset alpaca_gpt4_zh,identity,adgen_local \
    --dataset_dir ./data \
    --template llama3 \
    --finetuning_type lora \
    --output_dir /root/autodl-tmp/saves/LLaMA3-8B/lora/sft \
    --overwrite_cache \
    --overwrite_output_dir \
    --cutoff_len 1024 \
    --preprocessing_num_workers 16 \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --logging_steps 50 \
    --warmup_steps 20 \
    --save_steps 100 \
    --eval_steps 50 \
    # --evaluation_strategy steps \
    # --save_strategy steps \
    # --load_best_model_at_end \
    --learning_rate 5e-5 \
    --num_train_epochs 5.0 \
    --max_samples 1000 \
    --val_size 0.1 \
    --plot_loss \
    --fp16
```

| å‚æ•°åç§°                    | å‚æ•°è¯´æ˜                                                                                                                                                                                                                                                                                      |
| --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| model_name_or_path          | æ¨¡å‹çš„åç§°ï¼ˆhuggingfaceæˆ–è€…modelscopeä¸Šçš„æ ‡å‡†å®šä¹‰ï¼Œå¦‚â€œmeta-llama/Meta-Llama-3-8B-Instructâ€ï¼‰ï¼Œ æˆ–è€…æ˜¯æœ¬åœ°ä¸‹è½½çš„ç»å¯¹è·¯å¾„ï¼Œå¦‚/root/autodl-tmp/model/cache/LLM-Research/Meta-Llama-3-8B-Instruct                                                                                                 |
| template                    | æ¨¡å‹é—®ç­”æ—¶æ‰€ä½¿ç”¨çš„promptæ¨¡æ¿ï¼Œä¸åŒæ¨¡å‹ä¸åŒï¼Œè¯·[å‚è€ƒ](https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#supported-models)è·å–ä¸åŒæ¨¡å‹çš„æ¨¡æ¿å®šä¹‰ï¼Œå¦åˆ™ä¼šå›ç­”ç»“æœä¼šå¾ˆå¥‡æ€ªæˆ–å¯¼è‡´é‡å¤ç”Ÿæˆç­‰ç°è±¡çš„å‡ºç°ã€‚chat ç‰ˆæœ¬çš„æ¨¡å‹åŸºæœ¬éƒ½éœ€è¦æŒ‡å®šï¼Œæ¯”å¦‚Meta-Llama-3-8B-Instructçš„template å°±æ˜¯ llama3 |
| stage                       | å½“å‰è®­ç»ƒçš„é˜¶æ®µï¼Œæšä¸¾å€¼ï¼Œæœ‰â€œsftâ€,"pt","rm","ppo"ç­‰ï¼Œä»£è¡¨äº†è®­ç»ƒçš„ä¸åŒé˜¶æ®µï¼Œè¿™é‡Œæˆ‘ä»¬æ˜¯æœ‰ç›‘ç£æŒ‡ä»¤å¾®è°ƒï¼Œæ‰€ä»¥æ˜¯sft                                                                                                                                                                                  |
| do_train                    | æ˜¯å¦æ˜¯è®­ç»ƒæ¨¡å¼                                                                                                                                                                                                                                                                                |
| dataset                     | ä½¿ç”¨çš„æ•°æ®é›†åˆ—è¡¨ï¼Œæ‰€æœ‰å­—æ®µéƒ½éœ€è¦æŒ‰ä¸Šæ–‡åœ¨data_info.jsoné‡Œæ³¨å†Œï¼Œå¤šä¸ªæ•°æ®é›†ç”¨","åˆ†éš”                                                                                                                                                                                                             |
| dataset_dir                 | æ•°æ®é›†æ‰€åœ¨ç›®å½•ï¼Œè¿™é‡Œæ˜¯ dataï¼Œä¹Ÿå°±æ˜¯é¡¹ç›®è‡ªå¸¦çš„dataç›®å½•                                                                                                                                                                                                                                         |
| finetuning_type             | å¾®è°ƒè®­ç»ƒçš„ç±»å‹ï¼Œæšä¸¾å€¼ï¼Œæœ‰"lora","full","freeze"ç­‰ï¼Œè¿™é‡Œä½¿ç”¨lora                                                                                                                                                                                                                              |
| output_dir                  | è®­ç»ƒç»“æœä¿å­˜çš„ä½ç½®                                                                                                                                                                                                                                                                            |
| cutoff_len                  | è®­ç»ƒæ•°æ®é›†çš„é•¿åº¦æˆªæ–­                                                                                                                                                                                                                                                                          |
| per_device_train_batch_size | æ¯ä¸ªè®¾å¤‡ä¸Šçš„batch sizeï¼Œæœ€å°æ˜¯1ï¼Œå¦‚æœGPU æ˜¾å­˜å¤Ÿå¤§ï¼Œå¯ä»¥é€‚å½“å¢åŠ                                                                                                                                                                                                                                |
| fp16                        | ä½¿ç”¨åŠç²¾åº¦æ··åˆç²¾åº¦è®­ç»ƒ                                                                                                                                                                                                                                                                        |
| max_samples                 | æ¯ä¸ªæ•°æ®é›†é‡‡æ ·å¤šå°‘æ•°æ®                                                                                                                                                                                                                                                                        |
| val_size                    | éšæœºä»æ•°æ®é›†ä¸­æŠ½å–å¤šå°‘æ¯”ä¾‹çš„æ•°æ®ä½œä¸ºéªŒè¯é›†                                                                                                                                                                                                                                                    |

è®­ç»ƒå®Œä¼šä¿å­˜ä¸€äº›å†…å®¹ï¼š
- adapterå¼€å¤´çš„å°±æ˜¯LoRAä¿å­˜çš„ç»“æœäº†ï¼Œåç»­ç”¨äºæ¨¡å‹æ¨ç†èåˆ
- training_losså’Œtrainer_logç­‰è®°å½•äº†è®­ç»ƒçš„è¿‡ç¨‹æŒ‡æ ‡
- å…¶ä»–æ˜¯è®­ç»ƒå½“æ—¶å„ç§å‚æ•°çš„å¤‡ä»½

<details>

<summary>è®­ç»ƒè¯¦ç»†ç»“æœå¦‚ä¸‹</summary>

<br>

è®­ç»ƒå®Œä¼šåœ¨`/root/autodl-tmp/saves`ä¿å­˜ä»¥ä¸‹å†…å®¹ï¼š

```bash
.
`-- LLaMA3-8B
    `-- lora
        `-- sft
            |-- README.md
            |-- adapter_config.json
            |-- adapter_model.safetensors
            |-- all_results.json
            |-- chat_template.jinja
            |-- checkpoint-100
            |   |-- ...
            |-- checkpoint-200
            |   |-- ...
            |-- checkpoint-300
            |   |-- ...
            |-- checkpoint-310
            |   |-- README.md
            |   |-- adapter_config.json
            |   |-- adapter_model.safetensors
            |   |-- chat_template.jinja
            |   |-- optimizer.pt
            |   |-- rng_state.pth
            |   |-- scaler.pt
            |   |-- scheduler.pt
            |   |-- special_tokens_map.json
            |   |-- tokenizer.json
            |   |-- tokenizer_config.json
            |   |-- trainer_state.json
            |   `-- training_args.bin
            |-- special_tokens_map.json
            |-- tokenizer.json
            |-- tokenizer_config.json
            |-- train_results.json
            |-- trainer_log.jsonl
            |-- trainer_state.json
            |-- training_args.bin
            `-- training_loss.png
```

å…¶ä¸­ç»“æœå›¾åƒï¼š

![training loss](../images/2025/20250626_llama_sft_lora_training_loss.png)

ç»ˆç«¯è¾“å‡ºï¼š

```bash
[INFO|tokenization_utils_base.py:2356] 2025-06-26 11:20:01,079 >> chat template saved in /root/autodl-tmp/saves/LLaMA3-8B/lora/sft/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-06-26 11:20:01,081 >> tokenizer config file saved in /root/autodl-tmp/saves/LLaMA3-8B/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-06-26 11:20:01,082 >> Special tokens file saved in /root/autodl-tmp/saves/LLaMA3-8B/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =        5.0
  total_flos               = 38889053GF
  train_loss               =     2.2415
  train_runtime            = 0:16:08.44
  train_samples_per_second =      5.065
  train_steps_per_second   =       0.32
Figure saved at: /root/autodl-tmp/saves/LLaMA3-8B/lora/sft/training_loss.png
[WARNING|2025-06-26 11:20:01] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-06-26 11:20:01] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|modelcard.py:450] 2025-06-26 11:20:01,398 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
```

`adapter_config.json`å†…å®¹ï¼š

```bash
{
  "alpha_pattern": {},
  "auto_mapping": null,
  "base_model_name_or_path": "/root/autodl-tmp/model/cache/LLM-Research/Meta-Llama-3-8B-Instruct",
  "bias": "none",
  "corda_config": null,
  "eva_config": null,
  "exclude_modules": null,
  "fan_in_fan_out": false,
  "inference_mode": true,
  "init_lora_weights": true,
  "layer_replication": null,
  "layers_pattern": null,
  "layers_to_transform": null,
  "loftq_config": {},
  "lora_alpha": 16,
  "lora_bias": false,
  "lora_dropout": 0.0,
  "megatron_config": null,
  "megatron_core": "megatron.core",
  "modules_to_save": null,
  "peft_type": "LORA",
  "r": 8,
  "rank_pattern": {},
  "revision": null,
  "target_modules": [
    "down_proj",
    "q_proj",
    "v_proj",
    "k_proj",
    "o_proj",
    "up_proj",
    "gate_proj"
  ],
  "task_type": "CAUSAL_LM",
  "trainable_token_indices": null,
  "use_dora": false,
  "use_rslora": false
}
```

`trainer_log.jsonl`å†…å®¹ï¼š

```bash
{"current_steps": 50, "total_steps": 310, "loss": 2.8631, "lr": 4.877641290737884e-05, "epoch": 0.814663951120163, "percentage": 16.13, "elapsed_time": "0:02:37", "remaining_time": "0:13:37"}
{"current_steps": 100, "total_steps": 310, "loss": 2.3445, "lr": 4.139011743862991e-05, "epoch": 1.6191446028513239, "percentage": 32.26, "elapsed_time": "0:05:11", "remaining_time": "0:10:54"}
{"current_steps": 150, "total_steps": 310, "loss": 2.1983, "lr": 2.9311566583660317e-05, "epoch": 2.423625254582485, "percentage": 48.39, "elapsed_time": "0:07:47", "remaining_time": "0:08:18"}
{"current_steps": 200, "total_steps": 310, "loss": 2.1012, "lr": 1.5998676096837534e-05, "epoch": 3.2281059063136457, "percentage": 64.52, "elapsed_time": "0:10:23", "remaining_time": "0:05:42"}
{"current_steps": 250, "total_steps": 310, "loss": 2.0219, "lr": 5.262735453472459e-06, "epoch": 4.032586558044806, "percentage": 80.65, "elapsed_time": "0:12:58", "remaining_time": "0:03:06"}
{"current_steps": 300, "total_steps": 310, "loss": 1.9775, "lr": 1.7729037394193793e-07, "epoch": 4.84725050916497, "percentage": 96.77, "elapsed_time": "0:15:37", "remaining_time": "0:00:31"}
{"current_steps": 310, "total_steps": 310, "epoch": 5.0, "percentage": 100.0, "elapsed_time": "0:16:08", "remaining_time": "0:00:00"}
```

`train_results.json`è·Ÿ`all_results.json`å†…å®¹ï¼š

```bash
{
    "epoch": 5.0,
    "total_flos": 4.175680284603187e+16,
    "train_loss": 2.2414812703286446,
    "train_runtime": 968.4429,
    "train_samples_per_second": 5.065,
    "train_steps_per_second": 0.32
}
```

`special_tokens_map.json`å†…å®¹ï¼š

```bash
{
  "additional_special_tokens": [
    {
      "content": "<|eom_id|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false
    }
  ],
  "bos_token": {
    "content": "<|begin_of_text|>",
    "lstrip": false,
    "normalized": false,
    "rstrip": false,
    "single_word": false
  },
  "eos_token": {
    "content": "<|eot_id|>",
    "lstrip": false,
    "normalized": false,
    "rstrip": false,
    "single_word": false
  },
  "pad_token": "<|eot_id|>"
}
```

</details>


## åŠ¨æ€åˆå¹¶LoRAçš„æ¨ç†

å½“åŸºäºLoRAçš„è®­ç»ƒè¿›ç¨‹ç»“æŸåï¼Œæˆ‘ä»¬å¦‚æœæƒ³åšä¸€ä¸‹åŠ¨æ€éªŒè¯ï¼Œåœ¨ç½‘é¡µç«¯é‡Œä¸æ–°æ¨¡å‹å¯¹è¯ï¼Œä¸åŸå§‹æ¨¡å‹ç›´æ¥æ¨ç†ç›¸æ¯”ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯éœ€è¦é€šè¿‡finetuning_typeå‚æ•°å‘Šè¯‰ç³»ç»Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨äº†LoRAè®­ç»ƒï¼Œç„¶åå°†LoRAçš„æ¨¡å‹ä½ç½®é€šè¿‡`adapter_name_or_path`å‚æ•°å³å¯ã€‚å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli webchat \
    --model_name_or_path /root/autodl-tmp/model/cache/LLM-Research/Meta-Llama-3-8B-Instruct \
    --adapter_name_or_path /root/autodl-tmp/saves/LLaMA3-8B/lora/sft  \
    --template llama3 \
    --finetuning_type lora
```

æˆ‘æ‡’å¾—å†åšç«¯å£æ˜ å°„ï¼Œç„¶åå»ç½‘é¡µæµ‹è¯•ï¼Œæ‰€ä»¥ç›´æ¥ç”¨ç»ˆç«¯çš„æ–¹å¼æµ‹è¯•ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli chat \
    --model_name_or_path /root/autodl-tmp/model/cache/LLM-Research/Meta-Llama-3-8B-Instruct \
    --adapter_name_or_path /root/autodl-tmp/saves/LLaMA3-8B/lora/sft  \
    --template llama3 \
    --finetuning_type lora
```

ç»“æœå¦‚ä¸‹ï¼š

```bash
[INFO|2025-06-26 11:48:51] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-26 11:48:52] llamafactory.model.adapter:143 >> Merged 1 adapter(s).
[INFO|2025-06-26 11:48:52] llamafactory.model.adapter:143 >> Loaded adapter(s): /root/autodl-tmp/saves/LLaMA3-8B/lora/sft
[INFO|2025-06-26 11:48:52] llamafactory.model.loader:143 >> all params: 8,030,261,248
Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.

User: ä½ æ˜¯è°
Assistant: æ‚¨å¥½ï¼Œæˆ‘æ˜¯ DamonzhengBotï¼Œæ˜¯ç”± LLaMA Factory-damonzheng å¼€å‘çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯å›ç­”ç”¨æˆ·çš„é—®é¢˜å¹¶æä¾›å¸®åŠ©ã€‚

User: ç±»å‹#è£™*è£™æ¬¾å¼#é“¾æ¡
Assistant: è£™èº«ä¸¤ä¾§è£…é¥°æœ‰é“¾æ¡ï¼Œå¸¦æ¥ä¸€ç§æ‘©ç™»æ„Ÿã€‚ç®€æ´çš„ç‰ˆå‹ï¼Œæ­é…ç²¾è‡´çš„é¢æ–™ï¼Œç©¿å‡ºä¼˜é›…çš„æ°”è´¨ã€‚

User:
```

å¦‚æœç”¨åŸæ¥çš„å¾®è°ƒä¹‹å‰çš„å¤§æ¨¡å‹ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli chat \
    --model_name_or_path /root/autodl-tmp/model/cache/LLM-Research/Meta-Llama-3-8B-Instruct \
    --template llama3
```

ç»“æœå°±æ˜¯è¿™æ ·ï¼š

```bash
[INFO|2025-06-26 12:57:32] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-26 12:57:32] llamafactory.model.loader:143 >> all params: 8,030,261,248
Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.

User: ä½ æ˜¯è°
Assistant: ğŸ˜Š

I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm a large language model trained on a massive dataset of text from the internet, which allows me to generate human-like responses to a wide range of topics and questions.

I don't have a personal identity or physical existence, but I'm here to assist and communicate with you in a helpful and informative way. I can answer questions, provide information, offer suggestions, and even engage in fun conversations and games. So, feel free to ask me anything or start a conversation - I'm here to help! ğŸ’¬

User: ç±»å‹#è£™*è£™æ¬¾å¼#é“¾æ¡
Assistant: You're interested in dresses and dress styles with chain details! ğŸ’ƒ

Here are some popular types of dresses with chain details:

1. **Chain Belt Dress**: A dress with a chain belt around the waist, adding a touch of glamour and sophistication.
2. **Chain Link Dress**: A dress with chain links as a decorative element, often used to create a statement piece or add texture to the garment.
3. **Chain Shoulder Dress**: A dress with chain details on the shoulders, adding a touch of edginess and modernity.
4. **Chain Tassel Dress**: A dress with chain tassels or fringe, adding a playful and whimsical touch.
5. **Chain Embellished Dress**: A dress with chain embellishments, such as chain sequins or chain beading, adding a touch of luxury and glamour.

Some popular dress styles with chain details include:

1. **Shift Dress**: A simple, straight-cut dress that can be elevated with chain details.
2. **Fit-and-Flare Dress**: A dress that hugs the body and flares out at the hem, perfect for showcasing chain details.
3. **Maxi Dress**: A long, flowing dress that can be dressed up or down with chain details.
4. **Bodycon Dress**: A tight-fitting dress that can be made more interesting with chain details.
5. **Asymmetrical Dress**: A dress with an asymmetrical hemline or neckline, perfect for showcasing chain details.

These are just a few examples, but I hope this helps you get started on your search for the perfect chain-detailed dress! ğŸ’ƒ

User:
```

## ä¸€ç«™å¼webui boardçš„ä½¿ç”¨

è¿™ä¸ªboardå°†è®­ç»ƒå¤§æ¨¡å‹ä¸»è¦çš„é“¾è·¯å’Œæ“ä½œéƒ½åœ¨ä¸€ä¸ªé¡µé¢ä¸­è¿›è¡Œäº†æ•´åˆï¼Œæ‰€æœ‰å‚æ•°éƒ½å¯ä»¥å¯è§†åŒ–åœ°ç¼–è¾‘å’Œæ“ä½œ

```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli webui
# å¦‚æœè¦å¼€å¯ gradioçš„shareåŠŸèƒ½ï¼Œæˆ–è€…ä¿®æ”¹ç«¯å£å·
CUDA_VISIBLE_DEVICES=0 GRADIO_SHARE=1 GRADIO_SERVER_PORT=7860 llamafactory-cli webui
```

![llamafactoryçš„webui](../images/2025/20250626_llamafactoryçš„webui.png)


## å…¶ä»–

è‡³äºåç»­çš„ RM è·Ÿ PPO è¿™è¾¹å°±ä¸ä¸¾ä¾‹äº†ã€‚ä¸»è¦å¯ä»¥å»çœ‹çœ‹æ•°æ®é›†çš„å½¢å¼ã€‚

å‘½ä»¤çš„ä¸€äº›å‚æ•°ï¼š

```bash
llamafactory-cli train -h   # ä½¿ç”¨è¿™ä¸ªå‘½ä»¤æŸ¥çœ‹
```