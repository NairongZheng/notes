# 机器学习2



- [线性回归1](https://blog.csdn.net/weixin_44697198/article/details/109405212).
    - [最小二乘法](https://blog.csdn.net/ccnt_2012/article/details/81127117).
      - [联合概率分布](https://blog.csdn.net/qq_39636214/article/details/85036837).
      - [极大似然估计](https://www.zhihu.com/question/24124998).
      - [正态分布](https://blog.csdn.net/ccnt_2012/article/details/81327881).
- [线性回归2](https://zhuanlan.zhihu.com/p/147297924).
    - [梯度下降](https://zhuanlan.zhihu.com/p/68468520).
    - [正则化](https://blog.csdn.net/qq_46092061/article/details/119680604).
- [逻辑回归](https://www.bilibili.com/video/BV1PJ411676g).
- [决策树](https://blog.csdn.net/lys_828/article/details/108669442).
    - [GBDT](https://blog.csdn.net/zpalyq110/article/details/79527653).
        - [CART分类树](https://zhuanlan.zhihu.com/p/139523931).
        - CART回归树[1](https://blog.csdn.net/sun_xiao_kai/article/details/88948356), [2](https://www.cnblogs.com/limingqi/p/12421960.html).
    - [随机森林](https://zhuanlan.zhihu.com/p/406627649).
- [SVM](https://zhuanlan.zhihu.com/p/49331510).
    - [拉格朗日对偶](https://zhuanlan.zhihu.com/p/38182879).
    - [线性可分与不可分](https://blog.csdn.net/why19940926/article/details/78720641).
- [K-means聚类](https://zhuanlan.zhihu.com/p/78798251).
- [KNN](https://zhuanlan.zhihu.com/p/143092725).



1. 线性回归2：

   （1）“线性”限制的是parameter（参数），而不是feature（自变量）。

   （2）线性回归要做的是就是找到一个数学公式能相对较完美地把所有自变量组合（加减乘除）起来，得到的结果和目标接近。

   （3）所以线性的定是：**自变量之间只存在线性关系**，即自变量只能通过相加、或者相减进行组合。

2. 逻辑回归：

   （1）逻辑回归=线性回归+sigmoid函数

   （2）逻辑回归是用来做分类的！！！

   （3）神经网络中每个节点其实都是一个逻辑回归。

3. 随机森林两个“随机”：

   （1）如果训练集的大小为N，对每棵树而言，随机且有放回地从训练集中抽取N个训练样本作为该树的训练集。这样，每个树训练集都是不同的，而且里面包含重复的训练样本。
   
   （2）如果有M个特征，在每个节点分裂的时候，从M中随机选择m个特征维度（m<<M），使用这些m个特征维度中最佳特征（最大化信息增益）来分割节点，在森林生长期间，m保持不变。
   （参考链接写的非常好，好好看）
