## 深度学习基础

<details>
<summary>查全率、查准率、PR曲线、ROC曲线</summary>

<br>

<table>
    <tr align='center'>
        <th rowspan ='2'>真实情况</th>
        <th colspan ='2'>预测结果</th>
    </tr>
    <tr align='center'>
        <th colspan ='1'>正例</th>
        <th colspan ='1'>反例</th>
    </tr>
    <tr  align='center'>
        <td>正例</td>
		<td>TP(真正例)</td>
        <td>FN(假反例)</td>
    </tr>
    <tr  align='center'>
        <td>反例</td>
		<td>FP(假正例)</td>
        <td>TN(真反例)</td>
    </tr>
</table>

- **准确率（Accuracy）**：对于给定的测试数据集，分类正确的样本数与总样本数之比
  
$$
\frac{TP+TN}{总样本数}
$$

- **精确率/查准率（Precision）**：预测为正的样本中，又多少是真正的正样本

$$
\frac{TP}{TP+FP}
$$

- **召回率/查全率（Recall）**：样本中有多少正例被预测正确了

$$
\frac{TP}{TP+FN}
$$


**查准率和查全率是一对矛盾的度量**。

**PR曲线**：以查准率为纵轴、查全率为横轴作图 ，就得到了查准率-查全率曲线。

![](../images/20220702/20220702_1_机器学习.jpg)

**ROC曲线**：以​​假正率（FPR）​​为横轴，​​真正率（TPR）​​为纵轴，反映模型在不同阈值下的分类性能。

| 维度​          | ​	​​ROC曲线​                     | ​	​​PR曲线​​                     |
| -------------- | -------------------------------- | -------------------------------- |
| ​横轴​​	​​     | 假正率（FPR）	​​                 | 召回率（Recall）                 |
| ​纵轴​​​       | ​	真正率（TPR）​​                | 精确率（Precision）              |
| ​敏感度​​	​​   | 对类别平衡数据更敏感	​​          | 对类别不平衡数据更敏感           |
| ​典型场景​​	​​ | 医疗诊断、金融风控（平衡数据）​​ | 欺诈检测、推荐系统（正样本极少） |
| ​AUC意义​​	​​  | ROC-AUC越高，整体分类性能越好​​  | PR-AUC越高，正样本识别能力越强   |

</details>


## 大模型相关

<details>
<summary>transformer 中为什么使用 layer normalization 而不是用 batch normalization</summary>

<br>

1. 对批次大小的敏感性​​
   1. ​批归一化（BN）​​：依赖于当前批次的统计量（均值和方差），​**在​小批次或批次大小变化时​​表现不稳定**。例如，在自然语言处理（NLP）任务中，由于**句子长度不同**，常需动态填充（padding）或截断，导致批次内有效样本数不一致，影响BN的统计量计算。
   2. ​层归一化（LN）​​：对​**​单个样本的所有特征维度​​计算统计量**，与批次无关。无论批次大小如何，LN始终能稳定归一化，更适合Transformer中变长序列和动态批次的场景。
   3. 大模型训练时，多机多卡情况下，BN还有通信消耗。
2. Transformer处理的是​**​​序列数据​**​​（如文本中的单词），其自注意力机制使得​**每个位置的输出依赖于所有其他位置​**。此时：
   1. ​BN的缺陷​​：若对整个批次的不同位置计算统计量，​**不同样本间的依赖关系可能引入噪声，破坏局部模式​**。
   2. ​LN的优势​​：对同一序列内的所有位置独立归一化，​**保留序列内部的一致性​**，避免跨样本的信息干扰。
3. 训练与推理的一致性​​
   1. ​​​BN在推理阶段​需要维护全局的移动平均统计量，而​​**​训练阶段的批次统计量可能与推理阶段分布不同​**​​（尤其在小批次或在线学习时），导致不一致。
   2. ​LN无此问题​​：归一化仅依赖当前样本的特征，训练与推理行为完全一致，简化了部署流程。
4. 位置编码的兼容性​​
   1. Transformer依赖位置编码（Positional Encoding）注入序列顺序信息。若使用BN，不同位置的统计量可能被混合，削弱位置信息的作用；而​**​LN仅在单个序列内操作，保留了位置编码的独立性​**​。

| 特性                | 层归一化（LN）                       | 批归一化（BN）                  |
| ------------------- | ------------------------------------ | ------------------------------- |
| ​统计量计算范围​​   | 单个样本的所有特征                   | 当前批次的所有样本的同一特征    |
| ​依赖批次大小​​     | 否                                   | 是                              |
| ​处理变长序列​​     | 更稳定                               | 需填充/掩码，可能引入噪声       |
| ​训练与推理一致性​​ | 完全一致                             | 需维护移动平均，可能不一致      |
| ​适用场景​​         | 序列模型（Transformer、RNN）、小批次 | 图像模型（CNN）、大批次稳定场景 |
</details>

<details>
<summary>self-attention 与 softmax</summary>

<br>

Q：查询矩阵（理解：搜索栏中输入的查询内容）

$$
Q=XW^Q
$$

K：键矩阵（理解：数据库中与Q相关的一组关键字）

$$
K=XW^K
$$

V：值矩阵（理解：系统通过计算，展示最匹配K的所对应的内容V）

$$
V=XW^V
$$

总的公式：

$$
Attention(Q,K,V)=softmax(\frac {QK^T}{\sqrt{d_k}})V
$$

Attention 就是将想要查询的 Q 与数据库中的 K 进行比较，一对一地测量它们之间的相似度，并最终从最高相似度顺序向下依次并排列索引到的 V。所以，也可以理解 Attention 为一个数据库查表的过程。

**流程解释**：

拿出一组多头自注意力来解释（**下面多头的图是不准确的！看最后的多头解释**）：

![](../images/20211125/20211125_TRM_MSHA2.png)

1. 先计算 Q 与 K 的转置的点积。
2. 点积的结果就是生成注意力矩阵（**上图**）。注意力矩阵的第一行就是第一个字c1与这六个字分别的相关程度。
3. 然后用SoftMax进行归一化，这样每个字跟其他所有字的注意力权重的和为1
4. 接着用注意力矩阵给V加权，就可以找到最相关的值。

**多头自注意力**

> 多头注意力机制就是对同一个输入，使用**不同的** Q、K、V 权重**进行多组注意力计算**，得到多个结果后拼接起来，再通过**线性变换融合为最终输出**。（所以上面画的图是不准确的！）

| 问题                     | 答案                                                                                                                                                                  |
| ------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 多头注意力计算量是否更大 | 是，确实增加了计算量，因为有h组QKV计算而不是一组<br>每一组都要做一次完整的 attention 运算<br>最后还要做一次拼接与线性映射                                             |
| 为什么计算量“增加但没炸” | 虽然用了多个 attention head，但**每个 head 的维度更小**，从而控制住了总计算量。<br>没有重复计算整份，每个 head 只负责**分工处理**一个低维空间，而不是全维度重复处理。 |
| 多头比单头效果更好       | 是，能捕捉多种语义关系，提升表达能力                                                                                                                                  |
| 多头效率低，难以训练     | 否，框架优化良好，都会对 multi-head attention 做高效并行化处理                                                                                                        |

<br>

**softmax**：

$$
softmax=\frac{e^{z_i}}{\sum_{j=1}^{n}{e^{z_j}}}
$$

</details>