## 深度学习基础

<details>
<summary>查全率、查准率、PR曲线、ROC曲线</summary>

<table>
    <tr align='center'>
        <th rowspan ='2'>真实情况</th>
        <th colspan ='2'>预测结果</th>
    </tr>
    <tr align='center'>
        <th colspan ='1'>正例</th>
        <th colspan ='1'>反例</th>
    </tr>
    <tr  align='center'>
        <td>正例</td>
		<td>TP(真正例)</td>
        <td>FN(假反例)</td>
    </tr>
    <tr  align='center'>
        <td>反例</td>
		<td>FP(假正例)</td>
        <td>TN(真反例)</td>
    </tr>
</table>

- **准确率（Accuracy）**：对于给定的测试数据集，分类正确的样本数与总样本数之比
  
$$
\frac{TP+TN}{总样本数}
$$

- **精确率/查准率（Precision）**：预测为正的样本中，又多少是真正的正样本

$$
\frac{TP}{TP+FP}
$$

- **召回率/查全率（Recall）**：样本中有多少正例被预测正确了

$$
\frac{TP}{TP+FN}
$$


**查准率和查全率是一对矛盾的度量**。

**PR曲线**：以查准率为纵轴、查全率为横轴作图 ，就得到了查准率-查全率曲线。

![](../images/20220702/20220702_1_机器学习.jpg)

**ROC曲线**：以​​假正率（FPR）​​为横轴，​​真正率（TPR）​​为纵轴，反映模型在不同阈值下的分类性能。

| 维度​          | ​	​​ROC曲线​                     | ​	​​PR曲线​​                     |
| -------------- | -------------------------------- | -------------------------------- |
| ​横轴​​	​​     | 假正率（FPR）	​​                 | 召回率（Recall）                 |
| ​纵轴​​​       | ​	真正率（TPR）​​                | 精确率（Precision）              |
| ​敏感度​​	​​   | 对类别平衡数据更敏感	​​          | 对类别不平衡数据更敏感           |
| ​典型场景​​	​​ | 医疗诊断、金融风控（平衡数据）​​ | 欺诈检测、推荐系统（正样本极少） |
| ​AUC意义​​	​​  | ROC-AUC越高，整体分类性能越好​​  | PR-AUC越高，正样本识别能力越强   |

</details>


## 大模型相关

<details>
<summary>transformer 中为什么使用 layer normalization 而不是用 batch normalization</summary>

1. 对批次大小的敏感性​​
   1. ​批归一化（BN）​​：依赖于当前批次的统计量（均值和方差），​**在​小批次或批次大小变化时​​表现不稳定**。例如，在自然语言处理（NLP）任务中，由于**句子长度不同**，常需动态填充（padding）或截断，导致批次内有效样本数不一致，影响BN的统计量计算。
   2. ​层归一化（LN）​​：对​**​单个样本的所有特征维度​​计算统计量**，与批次无关。无论批次大小如何，LN始终能稳定归一化，更适合Transformer中变长序列和动态批次的场景。
   3. 大模型训练时，多机多卡情况下，BN还有通信消耗。
2. Transformer处理的是​**​​序列数据​**​​（如文本中的单词），其自注意力机制使得​**每个位置的输出依赖于所有其他位置​**。此时：
   1. ​BN的缺陷​​：若对整个批次的不同位置计算统计量，​**不同样本间的依赖关系可能引入噪声，破坏局部模式​**。
   2. ​LN的优势​​：对同一序列内的所有位置独立归一化，​**保留序列内部的一致性​**，避免跨样本的信息干扰。
3. 训练与推理的一致性​​
   1. ​​​BN在推理阶段​需要维护全局的移动平均统计量，而​​**​训练阶段的批次统计量可能与推理阶段分布不同​**​​（尤其在小批次或在线学习时），导致不一致。
   2. ​LN无此问题​​：归一化仅依赖当前样本的特征，训练与推理行为完全一致，简化了部署流程。
4. 位置编码的兼容性​​
   1. Transformer依赖位置编码（Positional Encoding）注入序列顺序信息。若使用BN，不同位置的统计量可能被混合，削弱位置信息的作用；而​**​LN仅在单个序列内操作，保留了位置编码的独立性​**​。

| 特性                | 层归一化（LN）                       | 批归一化（BN）                  |
| ------------------- | ------------------------------------ | ------------------------------- |
| ​统计量计算范围​​   | 单个样本的所有特征                   | 当前批次的所有样本的同一特征    |
| ​依赖批次大小​​     | 否                                   | 是                              |
| ​处理变长序列​​     | 更稳定                               | 需填充/掩码，可能引入噪声       |
| ​训练与推理一致性​​ | 完全一致                             | 需维护移动平均，可能不一致      |
| ​适用场景​​         | 序列模型（Transformer、RNN）、小批次 | 图像模型（CNN）、大批次稳定场景 |
</details>

