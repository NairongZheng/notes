- [强化学习基础](#强化学习基础)
  - [简介](#简介)
  - [基础概念](#基础概念)
  - [其他概念](#其他概念)
- [强化学习算法](#强化学习算法)
  - [REINFORCE 算法](#reinforce-算法)
  - [Actor-Critic 方法](#actor-critic-方法)
  - [近端策略优化 (Proximal Policy Optimization, PPO)](#近端策略优化-proximal-policy-optimization-ppo)


# 强化学习基础

## 简介

强化学习（Reinforcement Learning, RL）是机器学习的一个重要分支，它关注智能体（Agent）如何在特定环境（Environment）中通过与环境的交互来学习最优行为策略，以最大化累积奖励。

与监督学习和无监督学习不同，强化学习没有明确的标签或预设的模式，而是通过“试错”的方式进行学习。智能体在环境中执行动作，环境会根据动作给出奖励或惩罚，智能体则根据这些反馈调整其行为，以期在未来获得更多的奖励。

强化学习的核心思想是序贯决策，即智能体需要在一系列时间步中做出决策，并且当前决策会影响未来的状态和奖励。这使得强化学习在机器人控制、游戏AI、推荐系统、自动驾驶等领域展现出巨大的潜力。

## 基础概念

**智能体、环境、状态、动作和奖励**

> 在强化学习中，有几个核心组成部分：
> 
> - **智能体 (Agent)**：学习和决策的实体，它通过与环境的交互来学习最优行为。
> - **环境 (Environment)**：智能体所处的外部世界，它接收智能体的动作并返回新的状态和奖励。
> - **状态 (State)**：环境在某一时刻的描述，它包含了智能体做出决策所需的所有相关信息。
> - **动作 (Action)**：智能体在给定状态下可以执行的操作。
> - **奖励 (Reward)**：环境对智能体动作的即时反馈，可以是正值（鼓励）或负值（惩罚）。智能体的目标是最大化长期累积奖励。

**策略 (Policy)**

> 策略 $\pi$ 是智能体的行为准则，它定义了**在给定状态下选择哪个动作的概率分布**。策略可以是确定性的（在每个状态下选择一个特定的动作），也可以是随机性的（在每个状态下以一定的概率选择不同的动作）。
> 
> 数学上，策略可以表示为：
> 
> $$
> \pi(a|s) = P(A_t = a | S_t = s)
> $$
> 
> 这表示在状态 $s$ 下选择动作 $a$ 的概率。

**价值函数 (Value Function)**

> 价值函数用于评估状态或状态-动作对的“好坏”，即从该状态或状态-动作对开始，遵循某一策略所能获得的未来累积奖励的期望。强化学习的目标就是找到一个最优策略，使得价值函数最大化。
> 
> 价值函数有：
> 1. 状态价值函数 (State-Value Function) $V^{\pi}(s)$
> 2. 动作价值函数 (Action-Value Function) $Q^{\pi}(s, a)$

**状态价值函数 (State-Value Function) $V^{\pi}(s)$**

> **状态价值函数 $V^{\pi}(s)$ 表示从状态 $s$ 开始，并遵循策略 $\pi$ 所能获得的期望累积奖励。它衡量了智能体处于某个状态的长期吸引力。**
> 
> $$
> V^{\pi}(s) = E_{\pi}[G_t | S_t = s] = E_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s]
> $$
> 
> 其中：
> - $G_t$ 是从时间步 $t$ 开始的未来累积奖励（Return）。
> - $R_{t+k+1}$ 是在时间步 $t+k+1$ 获得的奖励。
> - $\gamma \in [0, 1]$ 是折扣因子 (Discount Factor)，**它决定了未来奖励的重要性**。$\gamma$ 越接近0，智能体越关注即时奖励；$\gamma$ 越接近1，智能体越关注长期奖励。

**动作价值函数 (Action-Value Function) $Q^{\pi}(s, a)$**

> **动作价值函数 $Q^{\pi}(s, a)$ 表示在状态 $s$ 下执行动作 $a$，然后遵循策略 $\pi$ 所能获得的期望累积奖励。它衡量了在特定状态下执行特定动作的长期吸引力。**
> 
> $$
> Q^{\pi}(s, a) = E_{\pi}[G_t | S_t = s, A_t = a] = E_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a]
> $$

**从Q到V**

> **一个状态的V值，就是这个状态下的所有动作的Q值在策略下的期望**。
> $$
> V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^{\pi}(s, a)
> $$
> 
> 其中：
> - $V^{\pi}(s)$：V值
> - $\pi(a|s)$：策略
> - $Q^{\pi}(s, a)$：Q值
> - 这边跟策略有关，因为动作是由策略来决定的


**从V到Q**

> **​一个状态-动作对的 Q 值，等于在该状态下采取该动作后，获得的即时奖励加上未来所有可能状态的 V 值的期望。​**
> 
> $$
> Q^{\pi}(s,a)=R(s,a)+\gamma\sum_{s'}P(s'|s,a) V^{\pi}(s')
> $$
> 
> - $Q^{\pi}(s, a)$：Q值
> - $R(s,a)$：奖励
> - $\gamma$：折扣率，用于权衡当前奖励和未来奖励的重要性
> - $P(s'|s,a)$：状态转移概率，在状态$s$下采取动作$a$后转移到状态$s'$的概率（即环境动力学）
> - $V^{\pi}(s')$：V值
> - 这里不需要关注策略，这里是环境的状态转移概率决定的。
> - 当我们选择a，并转移到新的状态时，就能获得奖励，我们必须把这个**奖励也算上！**

## 其他概念

**Model-Based（基于模型的强化学习）​**

> **核心思想**
> ​Model-Based 方法 首先尝试学习或构建一个环境模型​（即环境的动态特性），然后利用这个模型进行规划（Planning）​，从而找到最优策略。
> 
> ​环境模型通常包括：
> - ​状态转移概率$P(s'|s,a)$
> - ​奖励函数$R(s,a)$
> 
> **​学习过程**
> - ​学习环境模型​（Model Learning）：通过交互数据（状态、动作、奖励、下一状态）估计$P(s'|s,a)$和$R(s,a)$。
> - ​基于模型进行规划​（Planning）：使用学到的模型（如动态规划、蒙特卡洛树搜索 MCTS 等）计算最优策略或价值函数。
> 
> **优点**
> - ​样本效率高：由于可以基于模型进行规划，不需要像无模型方法那样依赖大量真实交互数据。
> - ​可解释性强：模型可以提供对环境动态的理解，便于分析和调试。
> 
> **​缺点**
> - ​模型误差问题：如果学到的模型不准确，规划结果可能会偏离最优策略。
> - ​模型学习可能困难：某些复杂环境的状态转移和奖励函数难以准确建模。
> 
> **​典型算法**
> - ​Dyna-Q​（结合模型学习和无模型学习）
> - ​蒙特卡洛树搜索（MCTS）​​（如 AlphaGo 使用的方法）
> - ​基于模型的强化学习（MBRL）​​（如 PETS、MuZero 等）

**Value-Based（基于价值的强化学习）​**

> **核心思想**
> ​Value-Based 方法不直接学习策略，而是学习价值函数 Q 或 V，然后通过价值函数间接导出策略。
> 通常使用 ​Q-Learning 或 ​SARSA 等算法来估计最优 Q 值或 V 值。
> ​
> **学习过程**
> - ​估计价值函数：通过 Q-Learning、SARSA 等算法学习 Q 或 V。
> - ​从价值函数导出策略：例如，$\epsilon$-贪婪策略：以$\epsilon$概率随机探索，以$1-\epsilon$概率选择当前 Q 值最大的动作。
> 
> **​优点**
> - ​适用于离散动作空间：Value-Based 方法在离散动作空间中表现良好（如 Atari 游戏）。
> - ​理论基础扎实：基于 Bellman 方程，有严格的数学保证。
> 
> **​缺点**
> - ​不适用于连续动作空间：由于 Q 值是离散动作的函数，难以直接扩展到连续动作空间（如机器人控制）。
> - ​策略可能不稳定：由于策略是通过价值函数间接导出的，可能存在高方差问题。
> 
> **​典型算法**
> - ​Q-Learning
> - ​SARSA
> - Deep Q-Network (DQN)​（结合神经网络的 Q-Learning）

**Policy-Based（基于策略的强化学习）​**

> **核心思想**
> ​Policy-Based 方法 直接学习策略函数$\pi(a|s)$（即给定状态 s 下选择动作 a 的概率），而不是通过价值函数间接导出策略。
> 通常使用 ​策略梯度（Policy Gradient）​ 方法来优化策略。
> 
> **​学习过程**
> - ​直接参数化策略：策略$\pi(a|s;\theta)$通常用神经网络表示，参数为$\theta$。
> - ​通过策略梯度优化策略：使用 ​REINFORCE、Actor-Critic 等方法计算策略梯度，并更新参数$\theta$以最大化期望回报。
> 
> **​优点**
> - ​适用于连续动作空间：可以直接输出连续动作的概率分布（如高斯策略）。
> - ​策略改进更稳定：相比 Value-Based 方法，Policy-Based 方法通常更稳定，尤其是在高维或连续动作空间中。
> 
> **​缺点**
> - ​高方差问题：策略梯度方法通常具有较高的方差，可能导致训练不稳定。
> - ​样本效率较低：相比 Model-Based 方法，Policy-Based 方法通常需要更多的交互数据。
> 
> **​典型算法**
> - ​REINFORCE​（蒙特卡洛策略梯度）
> - ​Actor-Critic​（结合价值函数和策略梯度）
> - ​Proximal Policy Optimization (PPO)
> - ​Trust Region Policy Optimization (TRPO)

**混合方法（结合多种范式）​**

> 现代强化学习算法往往结合多种范式的优势：
> - ​Actor-Critic：结合 Policy-Based（Actor）和 Value-Based（Critic），既直接优化策略，又利用价值函数减少方差。
> - ​Model-Based RL + Policy Gradient：如 ​MuZero、PETS，结合模型学习和策略优化，提高样本效率。
> - ​Dyna-Q：结合模型学习和 Q-Learning，兼顾规划与探索。

**on-policy**

> 想象你是一个打游戏的 AI 学徒：
> - 你自己操控角色，打一局后回看录像总结哪里做得不好，然后优化自己的操作方式。
> - 自己打的局，自己学；
> - 每一局用的是“你现在的策略”，从中学习也是为了改进这个策略；
> - 比如：SARSA、PPO、REINFORCE。
> 
> | 特性       | 描述                                                          |
> | ---------- | ------------------------------------------------------------- |
> | 策略一致性 | 收集数据用的策略 == 学习/更新的策略                           |
> | 样本利用率 | 一般只能用一次（不适合重放）                                  |
> | 探索方式   | 通常要有策略自带的随机性（比如 epsilon-greedy，或者策略分布） |
> | 优点       | 理论收敛性强，学得稳定                                        |
> | 缺点       | 样本效率低，不能重复利用数据                                  |

**off-policy**

> 还是一个打游戏的 AI 学徒：
> - 你在旁边看大神打游戏（或者过去自己老版本的录像），从他们的操作中提取经验并优化自己的策略。
> - 用的是别人的经验，训练的是自己的策略；
> - 比如 Q-learning 中你可以随便探索（比如随机行动），但训练时总是更新“最优策略”对应的 Q 值；
> - DQN、Q-Learning、DDPG 都是 off-policy 算法。
> 
> | 特性       | 描述                                           |
> | ---------- | ---------------------------------------------- |
> | 策略不一致 | 收集数据的策略 ≠ 学习优化的策略                |
> | 样本利用率 | 可以反复使用历史数据（比如经验回放）           |
> | 探索方式   | 数据可以来自随机策略或旧策略                   |
> | 优点       | 样本效率高，能用“离线数据”训练                 |
> | 缺点       | 学习不稳定，尤其当行为策略与目标策略差别太大时 |

**on-Policy与off-Policy总结**

> | 对比维度         | On-Policy                  | Off-Policy                     |
> | ---------------- | -------------------------- | ------------------------------ |
> | 数据收集策略     | 当前正在优化的策略         | 可以是旧的、随机的或别人的策略 |
> | 数据使用效率     | 每条数据用一次             | 可以反复使用、经验回放         |
> | 是否需要探索机制 | 是，需要策略有随机性       | 不一定，行为策略可以另设       |
> | 学习的稳定性     | 稳定但效率低               | 效率高但容易发散               |
> | 应用例子         | PPO、REINFORCE、A2C、SARSA | DQN、Q-Learning、DDPG、SAC     |


# 强化学习算法

## REINFORCE 算法

> REINFORCE（Monte-Carlo Policy Gradient）算法是最早、最基础的策略梯度算法之一，它是一种蒙特卡洛方法，因为它需要完整的轨迹（从开始到结束）来计算回报并更新策略。
> 
> REINFORCE算法的目标是最大化期望回报：
> 
> $$
> J(\theta)=E_{\pi_\theta}[G_t]
> $$
> 
> **策略梯度定理 (Policy Gradient Theorem)**
> 
> 策略梯度定理是策略梯度方法的基础，它提供了一种计算策略梯度的方法，即使在没有环境模型的情况下也能进行：
> 
> $$
> \nabla_\theta J(\theta)=E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a_t|s_t) G_t]
> $$
> 
> 其中：
> - $\nabla_\theta J(\theta)$：是策略参数$\theta$对期望回报的梯度。
> - $\nabla_\theta \log \pi_\theta(a_t|s_t)$：策略函数对数概率的梯度，也被称为“分数函数”（score function）。它表示在状态$s_t$下采取动作$a_t$对策略参数的影响方向。
> - $G_t$：从时间步$t$开始的累积回报（Return）。
> 
> 这个定理的直观解释是：如果一个动作$a_t$导致了较高的累积回报$G_t$，那么我们就应该增加在状态$s_t$下选择动作$a_t$的概率；反之，如果导致了较低的回报，就应该降低其概率。

**REINFORCE 的优缺点**

> 优点：
> - **可以直接优化策略**：适用于连续动作空间的问题。
> - **无模型**：不需要知道环境的动态。
> - **收敛性**：在理论上可以收敛到局部最优。
> 
> 缺点：
> - **高方差**：由于使用蒙特卡洛采样来估计回报 $G_t$，导致梯度估计的方差很大，使得训练过程不稳定，收敛速度慢。
> - **效率低**：需要等待整个回合结束后才能进行一次更新，效率较低。
> - **对回报敏感**：如果回报 $G_t$ 的值域很大，可能会导致梯度更新不稳定。

## Actor-Critic 方法

> AC算法的核心思想是使用 Critic 估计的价值函数来替代 REINFORCE 中蒙特卡洛采样的累积回报 $G_t$，从而降低梯度估计的方差。**最常见的替代是使用时间差分误差（TD Error）作为优势函数（Advantage Function）的估计**。
> 
> **优势函数 (Advantage Function)**
> 
> 
> 优势函数衡量了在给定状态$s$下，采取某个动作$a$比平均情况（由状态价值函数$V^{\pi}(s)$表示）好多少。它的定义是：
> 
> $$
> A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)
> $$
> 
> 在AC算法中，**通常使用TD误差来近似优势函数**：
> 
> $$
> \delta_t = R_{t+1} + \gamma V(s_{t+1}) - V(s_t)
> $$
> 
> 其中 $V(s_t)$ 和 $V(s_{t+1})$ 是由 Critic 网络估计的状态价值。这个TD误差可以看作是当前动作相对于预期价值的“意外”收益，它能够有效地指导策略的更新。
> 
> Actor 的策略梯度更新公式变为：
> 
> $$
> \nabla_{\theta} J(\theta) \approx E_{\pi_{\theta}}[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \delta_t]
> $$
> 
> **Critic 的价值函数更新则通过最小化TD误差的平方来实现**，类似于Q-learning或SARSA的更新方式。

**Actor-Critic 的优缺点**

> 优点：
> - **降低方差**：通过引入 Critic 估计的价值函数，TD误差作为基线（baseline）有效地降低了策略梯度估计的方差，使得训练更加稳定。
> - **处理连续动作空间**：与REINFORCE一样，可以直接处理连续动作空间。
> - **在线学习**：可以进行单步更新，不需要等待整个回合结束，提高了学习效率。
> 
> 缺点：
> - **收敛性问题**：Actor 和 Critic 两个网络同时学习，可能导致训练不稳定，难以收敛。
> - **超参数敏感**：对学习率等超参数比较敏感。
> 
> 虽然 Actor-Critic 相较于 REINFORCE 提升了学习效率和稳定性，但它依然存在一个**核心问题**：策略更新不稳定，容易导致策略崩溃（Policy Collapse）或性能震荡。
> 
> 在 Actor-Critic 中，策略通过以下公式更新：
> 
> $$
> \nabla_{\theta} J(\theta) \approx E_{\pi_{\theta}}[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \delta_t]
> $$
> 
> 这种更新方式属于策略梯度的 on-policy 方法，每次更新都高度依赖当前策略 $\pi_{\theta}$。如果学习率过大或 TD 误差较大，可能导致策略更新幅度太大，从而：
> - 策略发生剧烈变化；
> - 学到的策略偏离原有轨迹，导致估值函数 $V(s)$ 失效；
> - 整体训练变得不稳定，甚至退化。

## 近端策略优化 (Proximal Policy Optimization, PPO)

综上，所以PPO的核心思想：限制策略更新的幅度

**方法一：Clipped Surrogate Objective（最常用）**

> PPO 引入了一个截断比值函数，定义新旧策略概率比（这个其实就是**重要性采样**中的修正因子）：
> 
> $$
> r_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
> $$
> 
> 原始策略梯度目标变成了：
> 
> $$
> L^{CLIP}(\theta)=E[\min(r_t(\theta) \hat{A_t}, clip(r_t(\theta),1-\epsilon,1+\epsilon) \hat{A_t})]
> $$
> 
> 解释：
> - 如果 $r_t(\theta)$ 在 $[1 - \epsilon, 1 + \epsilon]$ 区间内，使用原始目标；
> - 如果超出了这个范围，说明策略变化太大，使用截断值来约束更新；
> - $\epsilon$ 通常取 0.1 ~ 0.3。
> - 优点：不会大幅改变策略，但仍然能朝着 Advantage 的正方向优化。

**方法二：KL Penalty（原论文中也提到）**

> 另一种方式是向目标函数中添加一个 KL 散度惩罚项：
> 
> $$
> L^{KL}(\theta)=E[r_t(\theta) \hat{A_t}-\beta · D_{KL}[\pi_{old}(·|s_t)\parallel \pi_\theta(·|s_t)]]
> $$
> 
> - $\beta$ 是超参数，用于调节惩罚强度；
> - 后期版本一般使用 Clipped Objective，因为它在实际中更稳定、高效。

**PPO总损失函数**

> PPO 总损失函数一般如下：
> 
> $$
> L=L^{CLIP}+c_1·L^{value}-c_2·L^{entropy}
> $$

**额外介绍一下GAE**

> 核心目标：在偏差和方差之间取得更好的平衡，从而估计出更加准确且稳定的优势函数 Advantage。
> 
> **背景：为什么需要 GAE？**
> 
> 在 Actor-Critic 中，我们要用 Advantage Function 来指导策略更新：
> 
> $$
> A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)
> $$
> 
> **但在实际中我们没有 $Q(s_t, a_t)$ 的精确值**，因此我们通常使用以下方式估计：
> 
> 1. 蒙特卡洛（MC）估计（低偏差，高方差）：$A_t^{MC}=\sum_{l=0}^\infty \gamma^l r_{t+1}-V(s_t)$
> 2. 一步 TD 估计（高偏差，低方差）：$A_t^{TD(1)}=r_t+\gamma V(s_{t+1})-V(s_t)$
> 
> 这两者在偏差-方差之间各有优缺点。GAE 的提出，就是为了在这两者之间找到一个可控的中间地带。
> 
> **GAE核心思想**
> 
> GAE 通过引入一个新的衰减因子 $\lambda$，来构造 Advantage 的加权平均版本：
> 
> 时间差分误差（TD Error）：
> 
> $$
> \delta_t=r_t+\gamma V(s_{t+1})-V(s_t)
> $$
> 
> GAE Advantage：
> 
> $$
> \hat{A_t^{GAE(\gamma,\lambda)}}=\sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+1}
> $$
> 
> - 当 $\lambda = 1$，GAE 退化为蒙特卡洛估计；
> - 当 $\lambda = 0$，GAE 退化为一步 TD；
> - $\lambda \in (0,1)$ 提供了一个折中，使得估计更稳定、更新更平滑。
> 
> **GAE推导**
> 
> 基本 TD-error：
> 
> $$
> \delta_t=r_t+\gamma V(s_{t+1})-V(s_t)
> $$
> 
> 基于TD的1-step Advantage：
> 
> $$
> A_t^{(1)}=\delta_t
> $$
> 
> 基于TD的2-step Advantage：
> 
> $$
> A_t^{(2)}=\delta_t+\gamma \delta_{t+1}
> $$
> 
> 基于TD的3-step Advantage：
> 
> $$
> A_t^{(2)}=\delta_t+\gamma \delta_{t+1}+\gamma^2 \delta_{t+2}
> $$
> 
> 基于TD的n-step Advantage：
> 
> $$
> A_t^{(n)}=\sum_{l=0}^{n-1}\gamma^l\delta_{t+1}
> $$
> 
> GAE 就是把所有这些 n-step advantage 加权平均，权重按$\lambda$指数衰减：
> 
> $$
> A_t^{GAE(\lambda)}=(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}A_t^{(n)}
> $$
> 
> 把$A_t^{(n)}$展开后：
> 
> $$
> \hat{A_t^{GAE(\gamma,\lambda)}}=\sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+1}
> $$
> 
> **解读：为什么是$(\gamma \lambda)^l$**
> 
> 这个权重来自于两部分思想：
> 
> | 部分        | 含义                     | 控制作用                           |
> | ----------- | ------------------------ | ---------------------------------- |
> | $\gamma^l$  | 奖励的折扣               | 时间越远未来奖励影响越小           |
> | $\lambda^l$ | 优势误差估计的信任度衰减 | 步数越多误差可能越大，需要权重变小 |
> 
> 这实际上是在做一个 指数加权平均的优势估计，越远的 TD 误差参与程度越低。