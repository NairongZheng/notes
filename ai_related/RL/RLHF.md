
- [RLHF整体流程](#rlhf整体流程)
- [第一步SFT监督微调](#第一步sft监督微调)
- [第二步奖励模型训练](#第二步奖励模型训练)
- [第三步使用 PPO 进行强化学习微调](#第三步使用-ppo-进行强化学习微调)


# RLHF整体流程

RLHF（Reinforcement Learning with Human Feedback，人类反馈强化学习）

> **整体流程**
> 
> 1. 阶段一：监督微调（SFT），用人类写好的问答对训练模型
> 2. 阶段二：奖励模型（RM）训练，人类对多个答案进行排序，然后训练奖励模型
> 3. 阶段三：强化学习微调，使用PPO等强化学习算法优化语言模型，用奖励模型给出奖励信号
> 
> **阶段 1：监督微调（Supervised Fine-Tuning, SFT）**
> 
> | 内容 | 说明                                               |
> | ---- | -------------------------------------------------- |
> | 数据 | 人类专家写好的问答对                               |
> | 目标 | 让预训练模型更符合“人类语言风格”和“高质量问答习惯” |
> | 方法 | 类似于常规的监督学习（Cross Entropy Loss）         |
> | 输出 | 初步具有人类风格的语言模型                         |
> | 优点 | 快速提升质量                                       |
> | 缺点 | 人工标注成本高，样本覆盖有限                       |
> 
> **阶段 2：奖励模型训练（Reward Model Training）**
> 
> | 内容         | 说明                                                                    |
> | ------------ | ----------------------------------------------------------------------- |
> | 数据         | 给定一个 prompt，模型生成多个候选回答，人类标注员排序（例如 A > B > C） |
> | 目标         | 训练一个奖励模型（RM）预测人类排序偏好                                  |
> | 方法         | Pairwise ranking loss（比如使用 Bradley-Terry loss）                    |
> | **模型结构** | 一般是 GPT 模型尾部接一个线性奖励头 r(x)，预测一个标量得分              |
> | 优点         | 模型学会区分“更人类友好”的输出                                          |
> | 缺点         | 排序主观性高，人力密集                                                  |
> 
> **阶段 3：使用 PPO 进行强化学习（PPO Fine-Tuning）**
> 
> | 内容     | 说明                                                |
> | -------- | --------------------------------------------------- |
> | 算法     | 使用强化学习算法（如 PPO）对语言模型进行微调        |
> | **状态** | prompt（提示词）                                    |
> | **动作** | 生成的文本                                          |
> | 奖励     | 奖励模型给出的得分 r(x)                             |
> | 优化目标 | 最大化期望奖励，同时不偏离原始模型太远（KL 正则项） |
> | 优点     | 模型逐步朝着人类偏好优化                            |
> | 缺点     | 训练不稳定、计算资源消耗大                          |
> 
> （为什么状态跟动作是prompt跟生成的文本，下面会解释）

# 第一步SFT监督微调

> **问题解答一：输入输出的方式**
> 
> LLM 本质上是一个自回归模型，也就是：**预测下一个 token 的概率，条件是前面的 token 序列**。
> 
> 因此，LLM 的输入输出不是 “固定长度” 的，而是：
> 
> > 输入：一个 token 序列
> > 输出：每个 token 的**下一个 token 概率分布**
> 
> 例如有一个训练样本：
> 
> > Human: 你是谁？
> > Assistant: 我是AI模型。
> 
> 我们把这整段拼成一个连续的文本：
> 
> > [Human: 你是谁？\nAssistant: 我是AI模型。]
> 
> 然后做一个很自然的**监督目标**：让模型预测每个 token 的下一个 token，最大化整个序列的似然概率（也就是最小化交叉熵损失）
> 
> 所以，SFT 就像是 “提示+答案”的拼接训练：
> - 把整段拼接成模型的输入；
> - 然后监督模型预测后面的部分。
> 
> 也就是说，prompt 只是输入上下文，真正要预测的是 response。
> 
> 那这样的输入模型岂不是看到“标签”了？
> 
> - 不会的。虽然拼接了，但训练只会监督模型从 prompt 开始，逐 token 去生成 response 部分。
> - **通过遮蔽机制避免了模型“提前看见答案”**。
> 
> **问题解答二：损失的计算方式**
> 
> - 模型会逐 token 地预测下一个 token 的概率。
> - 交叉熵损失只在 response 部分的 token 上进行累计计算。
> - prompt 的 token 仅作为 context 输入，不参与损失计算。
> 
> **问题解答三：交叉熵怎么用在这里？**
> 
> 举个简单的例子：
> - 假设当前预测的位置是"我"，标签是"是"。
> - 模型输出了一个 token 概率分布，比如：`"是": 0.2, "的": 0.1, "谁": 0.05, "AI": 0.01, ...`
> - 真正的标签是 "是"，所以交叉熵计算的是 -log(0.2)。
> - 如果模型把 "的" 的概率设为 0.8，那就错了，交叉熵会很大；
> - 反过来，如果把 "是" 的概率设为 0.9，那交叉熵很小，模型就得分高。
> 
> **模型会给出整个词表token的概率，然后来跟标签token做损失。**
> 
> **问题解答四：模型内部怎么用词表？**
> 
> 1. 输入文本 -> tokenizer -> token ids："你好" -> [1543, 3210]
> 2. token ids -> embedding 向量：embedding table是一个`[vocab_size x hidden_dim]` 的矩阵
> 3. 预测阶段：模型输出logits（得分）是`[batch_size, vocab_size]`，用 softmax 转成概率分布
> 4. 输出 token id -> tokenizer -> 文本
>
> 正是在输出层，每一步都需对整个词表计算 softmax，这导致计算瓶颈
>
> **示例**
>
> 对比图像任务：
> > 图像输入: 一张猫的图片；标签: "猫" 类别（class_id=3）
> > 预测: 模型输出 [0.1, 0.7, 0.05, 0.15] -> 最大值对应类别为“狗”
> > 损失: CrossEntropy(pred, label)
>
> 而NLP中的SFT：
> > 输入: 整段 token 序列（含 prompt 和回复），如“请介绍一下机器学习。机器学习是一种人工智能技术，它让计算机可以从数据中学习...”
> > 目标: 让模型预测每个 token 的下一个 token（一般从回复开始）
> > 损失: 对 token[i+1] 做交叉熵损失
> 
> | 项目           | 图像任务                     | NLP 的 LLM SFT                   |
> | -------------- | ---------------------------- | -------------------------------- |
> | 标签形式       | 单个 label（类 ID）或 vector | 一段完整的回复文本（token 序列） |
> | 输入是否含标签 | ❌ 标签是独立的               | ✅ 标签作为序列拼接在输入里       |
> | 损失位置       | 直接在输出向量上             | 在预测序列 token 上              |
> | 是否自回归     | 否                           | 是（每个 token 预测下一个）      |


# 第二步奖励模型训练

> **问题解答一：为什么需要奖励模型？**
> 
> Reward Model **训练目标**是：让模型学会区分哪一个回答更好，即：给“更优的回答”打更高的分。（满足人类偏好）
> - SFT 虽然能让模型产生人类风格的回复，但仍无法判断“哪一个更好”；
> - 奖励模型（Reward Model, RM）通过人类标注偏好建立一个“评分器”，让我们可以量化生成的好坏；
> - 这个模型训练好之后，可以用来在第三步的 RL 中当作 reward function。
> 
> **问题解答二：RM 模型结构**
> 
> 模型结构
> - 一般采用与 SFT 相同的 LLM 架构；
> - 在 decoder 最后加一层线性头（Linear Head）当作 reward head，预测一个标量分数；
> 
> 注意：
> - RM 不是用来生成文本的，而是评估“一个回复好不好”的；
> - RM 不训练预测 token，而是对整段 response 打分；
> - RM 通常 freeze encoder 或部分层，仅训练 reward head。
> 
> **问题解答三：训练数据长什么样？**
> 
> 一条训练数据包括：
> - prompt: 用户提问；
> - multiple responses: 模型生成多个回答；
> - human preferences: 人类对这些回答做出偏好排序，如 A > B > C；
> 
> 通常我们只用 pairwise：任意两条的偏好关系，比如：
> - prompt: “如何提高学习效率？”
> - response_1: “制定计划，注重休息...”
> - response_2: “提高效率要靠吃饭...”
> - label: response_1 > response_2
> 
> **问题解答四：训练方式**
> 
> 一般使用 pairwise loss（比如 Bradley-Terry / RankNet 风格）：
> 给定两个 response A跟B，LLM对他们的打分是：
> - $r_A$：Reward Model 对回答 A 的打分
> - $r_B$：Reward Model 对回答 B 的打分
> 
> 损失是：
> 
> $$
> L=-\log{(\sigma{(r_A-r_B)})}
> $$
> 
> 其中：
> - $\sigma(x)=\frac{1}{1+e^{-x}}$是sigmoid函数


# 第三步使用 PPO 进行强化学习微调

> **问题解答一：为什么要做 RL 微调？**
> 
> - 即使有了 SFT 和 RM，模型仍可能偏离人类喜好（比如重复、啰嗦、不够精炼）；
> - 使用强化学习（如 PPO）引导模型行为，使其**优化“人类评分更高”的方向**；
> - 奖励信号由 Reward Model 提供。
> 
> **问题解答二：强化学习的设定**
> 
> - 状态$s$：prompt + 已生成部分（可看作上下文）；
> - 动作$a$：选择一个 token；
> - 策略$\pi(a|s)$：即LLM；
> - 奖励$r$：Reward Model 给出完整 response 的得分；
> - 目标：优化策略，让生成的序列得到更高 reward。
> 
> **问题解答三：第三步可不可以不用强化学习的方法**
> 
> PPO 太复杂、训练不稳定、成本高，RL-free 方法更易用、训练稳定、效果接近甚至超过 PPO。比如 DPO：
> 
> 核心思想：不需要 PPO，而是直接基于人类偏好数据做分类优化：
> - 输入：同一个 prompt 下的优选回答 A 和较差回答 B；
> - 模型：直接最大化模型对优选回答的 log-prob，最小化对较差回答的 log-prob。
> - 损失函数（以偏好为信号）：
> 
> $$
> L_{DPO}=-\log(\frac{exp(\beta · \Delta \log \pi)}{exp(\beta · \Delta \log \pi)+1})
> $$
> 
> 其中：
> - $\Delta \log \pi=\log \pi(A)-\log \pi(B)$
> - $\pi$是当前模型的概率，$\beta$是温度参数
> 
> 优点：
> - 不需要训练额外的奖励模型（可选）
> - 不需要 RL/PPO 框架
> - 保留对原始模型的 KL 约束
> - 易于实现、训练稳定
> 
> （DPO相关可以查看另一个文档）