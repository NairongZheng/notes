- [LLM 微调基本定义与流程](#llm-微调基本定义与流程)
- [LLM 常见微调方法](#llm-常见微调方法)
  - [全参数微调（Full Fine-tuning）](#全参数微调full-fine-tuning)
  - [参数高效微调方法（PEFT）](#参数高效微调方法peft)
    - [LoRA（Low-Rank Adaptation）](#loralow-rank-adaptation)
    - [Adapter 微调](#adapter-微调)
    - [Prompt-based 微调](#prompt-based-微调)
- [如何选择微调方式？](#如何选择微调方式)


# LLM 微调基本定义与流程

<details>
<summary>LLM 微调基本定义与流程</summary>

<br>

**定义**

> 大模型的微调（fine-tuning）是指在预训练模型（如 GPT、BERT、LLaMA 等）基础上，**使用特定任务或领域的数据**对模型进行**再次训练**，以适应具体应用场景的需求。

**基本流程**

> 1. 选择预训练模型：选择一个经过大规模语料训练的模型，如 LLaMA、GPT-3、BERT、ChatGLM 等。
> 2. 准备数据集：数据应针对具体任务，数据格式需与模型输入兼容，通常是 JSON 格式。
>    1. 文本分类：标签化的文本数据
>    2. 问答系统：问答对
>    3. 对话系统：多轮对话
>    4. 信息抽取：带标注的文本等
> 3. 选择微调方法：
>    1. 全参数微调（Full Fine-tuning）
>    2. 参数高效微调（PEFT）
> 4. 训练设置：
>    1. 超参数设置（学习率、批大小、训练轮次）
>    2. 设备选择（GPU/TPU，分布式训练）
> 5. 评估与部署：
>    1. 在验证集测试效果
>    2. 保存模型权重用于部署

</details>

# LLM 常见微调方法

## 全参数微调（Full Fine-tuning）

**定义**

在保持原有大模型结构不变的情况下，对模型的**所有参数**（包括 Transformer 的 attention、MLP、LayerNorm 等）进行梯度更新训练，使其适应新的下游任务。

**标准微调（Vanilla Fine-tuning）**

1. 加载预训练模型
2. 准备下游任务数据（分类/生成/问答/翻译等）
3. 使用任务 loss（如交叉熵）训练整个模型

**分层微调（Layer-wise Fine-tuning）**

并非所有层一起训练，而是：

1. 从顶层（靠近输出）开始微调
2. 然后逐步解冻中间层、底层

**冻结部分参数（Partial Freezing）**

1. 冻结部分模块，如 embedding 层、低层 Transformer
2. 只训练中高层

**学习率调度（Layer-wise Learning Rate Decay, LLRD）**

目标：为模型中**不同层设置不同的学习率**，通常是底层（靠近输入）较小，顶层（靠近输出）较大。

这样做可以更稳定地训练大模型，尤其是如 BERT、RoBERTa、LLaMA 这种“层次结构明显”的 Transformer 模型。

<details>
<summary>为什么要用 LLRD？</summary>

<br>

> 大模型中各层的语义含义不同：
> 
> | 层级              | 表达内容                 | 微调建议                 |
> | ----------------- | ------------------------ | ------------------------ |
> | 底层（input 近）  | 通用特征（如词法、句法） | 保留原始能力（学习率小） |
> | 中间层            | 半通用语义               | 适度调整                 |
> | 顶层（output 近） | 任务相关特征（如分类）   | 重点微调（学习率大）     |

</details>


<details>
<summary>LLRD 的核心思想</summary>

<br>

> 给模型的每一层分配不同的学习率，通常按照指数衰减：
> 
> $$
> lr_l = base\_lr \times (decay\_rate)^{L-l-1}
> $$
> 
> 其中：
> - $l$：当前层编号（从底到顶）
> - $L$：总层数
> - $base\_lr$：顶层的初始学习率
> - $decay\_rate$：通常是 0.8 ~ 0.95
> 
> 使用示例：
> 
> | 情况                    | 建议                            |
> | ----------------------- | ------------------------------- |
> | 模型层数多（12 层以上） | 强烈建议使用 LLRD，微调更稳定   |
> | 训练数据量少            | LLRD 可避免模型过度拟合底层参数 |
> | 使用低学习率训练        | 配合 LLRD 提升梯度利用效率      |
> | 与全参数微调搭配        | 非常适合，互补                  |


</details>

## 参数高效微调方法（PEFT）

### LoRA（Low-Rank Adaptation）

<details>
<summary>LoRA（Low-Rank Adaptation）</summary>

<br>

**LoRA 的核心思想**

> 用低秩矩阵近似表示参数的微调变化，只训练这部分低秩矩阵，而不更新原始模型参数。
> 
> 以 Transformer 中的线性层为例：
> 
> - 原始的线性层为一个权重矩阵 $W_0\in{\mathbb{R}^{d\times{k}}}$，输入为 $x\in{\mathbb{R}^k}$，输出为 $y=W_0x$
> - 微调的目标是让 $W_0$ 稍作变化：$W=W_0+\Delta{W}$
> - 问题在于直接训练 $\Delta{W}\in{\mathbb{R}^{d\times{k}}}$ 仍然很大
> 
> LoRA 的做法：
> 
> - 用两个低秩矩阵 $A\in{\mathbb{R}^{d\times{r}}}$、$B\in{\mathbb{R}^{r\times{k}}}$ 近似表示 $\Delta{W}$，即 $\Delta \approx AB$
> - $r\ll \min (d,k)$，所以参数量大大减少，从 $dk$ 减少到 $r(d+k)$
> - 同时冻结原始参数 $W_0$，只训练 $A$ 和 $B$。

**数学形式表达**

> 设原始线性变换为：
> 
> $$
> y=W_0x
> $$
> 
> 引入 LoRA 后，新的线性变换为：
> 
> $$
> y=W_0x+\Delta Wx=W_0x+ABx
> $$
> 
> 其中：
> 
> - $W_0\in{\mathbb{R}^{d\times{k}}}$：冻结的原始权重
> - $A\in{\mathbb{R}^{d\times{r}}}$：可训练的矩阵
> - $B\in{\mathbb{R}^{r\times{k}}}$：可训练的矩阵
> - $r$ 是秩，决定了压缩程度，通常 $r=4,8,16$ 等
> - 由于 $rank(AB)\leq r$，这就构成了一个低秩近似
> 
> 有时还会加入缩放因子 $\alpha$，使输出更稳定：
> 
> $$
> y=W_0x+\frac{\alpha}{r}ABx
> $$

**什么是低秩矩阵？为什么两个低秩矩阵可以表示原来的矩阵？**

> 矩阵的秩可以理解为**矩阵中包含的有效信息维度的数量**，也就是线性无关的行（或列）的最大数量。
> 
> - 对于一个 $d\times k$ 的矩阵 $W$，如果它的秩是 $r$，说明它实际上只包含 $r$ 维的有效信息。
> - 通俗点说，它“看上去”是 $d\times k$，但其实信息都集中在一个低维空间里。
> 
> 任意一个秩为 $r$ 的矩阵 $\Delta{W}\in{\mathbb{R}^{d\times{k}}}$，都可以分解为两个矩阵的乘积：
> 
> $$
> W=AB,其中 A\in{\mathbb{R}^{d\times{r}}}, B\in{\mathbb{R}^{r\times{k}}}
> $$
> 
> 这是线性代数中著名的定理，例如 **SVD**（奇异值分解）就可以用来得到最优的低秩近似。
> 
> 在 LoRA 中，我们不是去精确分解原始 $W$，而是让更新量 $\Delta W$，**从一开始就限制在一个低秩空间**（因为训练参数就这么多），这样保证微调是轻量的。

**训练与推理阶段**

> 训练阶段：冻结原始模型权重 $W_0$，只训练 $A$ 和 $B$，参数量显著减少，训练更快、占用更少内存。
> 推理阶段：可以选择将 $W=W_0+AB$ **合并**成一个矩阵推理；或者保持**分离**结构计算 $W_0x+ABx$
> 
> |      | 合并推理                                                                           | 分离推理                                                                       |
> | ---- | ---------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
> | 优点 | 推理更快，只需一次矩阵乘法。                                                       | 模块化，可以动态加载不同的 $A$ 和 $B$ 比如做多任务、多语言时切换 LoRA 更方便。 |
> | 缺点 | 如果任务很多、需要动态加载不同任务的 LoRA 参数，就必须为每个任务都构建一个新的 $W$ | 比合并方式慢一点（额外一次乘法和加法），占用更多内存。                         |


**模型中哪些矩阵要用LoRA，怎么选**

> 选择使用LoRA的依据：
> - 对模型表现最关键的层（如 attention 中的 Q、V）
> - 参数量大的层（线性层更适合低秩分解）
> - 在不同任务中变化比较大的层（更值得微调）
> 
> 比如对于原来的 Query 投影会变成：
> 
> $$
> Q = W_q x + \frac{\alpha}{r} A_q B_q x
> $$
> 
> 其中的 $A_q$ 和 $B_q$ 是 LoRA 添加的低秩模块，只有这两个是可训练参数。

**LoRA与原始权重参数是怎么共存的**

> **LoRA 不会替换或修改原始模型参数，而是“旁路地添加一个可学习的低秩偏置项”，在前向传播时与原始输出相加，在反向传播时只更新低秩参数。**
> 
> **LoRA 的核心价值是“旁路添加”而不是“替换”：**
> 
> 1. 原始模型就像高速公路
> 2. LoRA 是旁边加了一条可调节的辅道
> 3. 推理时，原始高速公路照旧跑车，LoRA 的辅道可以开关（合并、移除都行）
> 
> | 问题               | 说明                             |
> | ------------------ | -------------------------------- |
> | 是否修改原始参数 W | ❌ 不改                           |
> | 是否添加新参数     | ✅ 添加 BA，秩很低                |
> | 是否可独立部署     | ✅ 原始模型和 LoRA 参数可分开保存 |
> | 推理时是否可合并   | ✅ 可合并为新 W                   |
> | 是否替代原矩阵     | ❌ 不是替换，而是“添加扰动”       |

**LoRA优缺点与使用建议**

> | 优点                       | 说明                                                                                                   |
> | -------------------------- | ------------------------------------------------------------------------------------------------------ |
> | 显著减少训练参数量         | 只训练两个小矩阵，相比于数亿全量参数，节省训练开销巨大 （<1%）                                         |
> | 省显存                     | 不更新原模型参数，避免了反向传播中巨大的梯度缓存，显著降低显存使用                                     |
> | 模块化设计，便于部署与共享 | LoRA 参数是外挂模块，不影响原始模型权重，可重复使用，可在多个任务间共享底座模型，只传/存小模型参数即可 |
> | 支持热插拔与推理融合       | 推理时可“合并 LoRA”到原参数中，部署灵活高效，或动态开启/关闭 LoRA 模块进行 A/B 测试                    |
> | 训练更快，资源利用率更高   | 少参数可更快收敛；多个任务可并行训练多个 LoRA，不需要复制整个模型                                      |
> | 适合多任务微调或少样本学习 | 多个 LoRA 对应多个任务（如对话、摘要、情感分析），只需载入对应 LoRA 权重即可切换任务                   |
> 
> | 缺点                                   | 说明                                                                                                       |
> | -------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
> | 性能可能不如全参微调（尤其数据量足时） | LoRA 的表达能力受限于秩 r，如果任务复杂、训练数据丰富，LoRA 的最终表现略逊全参微调（特别是在领域外迁移时） |
> | 不适合所有模块                         | LoRA 通常插入到 Attention 的 Q、V 或 Linear 层，对于非 Transformer 架构或 CNN、RNN 等模型适配性差          |
> | 需要调试超参数（rank、alpha）          | 不同任务对 LoRA 的秩（r）和缩放因子（α）敏感，需要试验调整，缺乏通用默认值                                 |
> | 引入结构复杂性                         | 多个 LoRA 插件（多任务场景）管理复杂，部署流程需要适配 LoRA 合并或动态载入，可能增加工程复杂度             |
> | 对低层特征学习能力有限                 | 由于只在高层添加微调能力，如果任务强依赖于低层感知（如多模态、图像分割），LoRA 表现可能受限                |
> 
> | 场景                   | 是否推荐使用 LoRA | 理由                                  |
> | ---------------------- | ----------------- | ------------------------------------- |
> | 文本分类 / NLU 任务    | ✅ 推荐            | 高层语义表达，LoRA 表现良好           |
> | 文本生成 / 翻译任务    | ✅ 推荐            | 模型容量大时，LoRA 具备很强的效率优势 |
> | 小样本或私有数据微调   | ✅ 强烈推荐        | 快速上手，成本低，适合少样本          |
> | 多任务共享底座模型     | ✅ 非常适合        | 每个任务一组 LoRA 权重即可            |
> | 图像任务或低层特征重要 | ❌ 不推荐          | 不适合结构完全不同的模型              |
> | 大量标注数据且资源充足 | ✅ 全参微调优先    | 性能上限更高                          |


</details>


### Adapter 微调

<details>
<summary>Adapter 微调</summary>

<br>

**核心思想**

> 在预训练模型的每一层或若干层之间**插入一个小型的“Adapter 模块”**，并且只训练这些模块的参数，其它原始模型参数保持冻结不变。
> 
> 这边是加了一个**模块**，而LoRA则是加了个 $\Delta W$，这是很大的区别。

**Adapter 模块结构**

> Adapter 模块一般是一个小型 MLP，采用瓶颈结构：
> 
> $$
> Adapter(x)=x+W_{up}\times ReLU(W_{down}x)
> $$
> 
> 其中：
> - $W_{down}\in \mathbb{R}^{d\times r}$：降维矩阵（r 是 bottleneck 维度，如 8、16）
> - $W_{up}\in \mathbb{R}^{r\times d}$：升维矩阵
> - 输入/输出维度不变，因此可以与主干模型无缝集成
> - 使用残差连接（加法），避免破坏原始特征

**Adapter 优缺点**

> | 优点             | 说明                                                                    |
> | ---------------- | ----------------------------------------------------------------------- |
> | 极大减少训练参数 | 只需训练 Adapter，小模型存储部署方便                                    |
> | 保留原始知识     | 主干模型保持冻结，不破坏已有的预训练知识                                |
> | 适用于多任务训练 | 每个任务独立训练 Adapter，主干模型共享（避免灾难性遗忘）                |
> | 动态插拔         | 推理时可插入/卸载 Adapter，灵活部署                                     |
> | 易于集成现有框架 | HuggingFace Transformers 已广泛支持 Adapter 加载与微调（如 AdapterHub） |
> 
> | 缺点               | 说明                                                          |
> | ------------------ | ------------------------------------------------------------- |
> | 表达能力受限       | Adapter 参数少，对复杂任务拟合能力不如全参数微调              |
> | 插入位置敏感       | 插入在 Attention 后 or FFN 后，位置选择影响性能，需要实验调整 |
> | 超参数需调试       | bottleneck 尺寸、激活函数、残差连接方式等均需调优             |
> | 增加训练代码复杂性 | 不如普通微调“直接训练模型”那么直观                            |

**Adapter与LoRA对比！！！**

> | 对比项             | LoRA                                            | Adapter                              |
> | ------------------ | ----------------------------------------------- | ------------------------------------ |
> | 训练的对象         | $\Delta W$ （在已有权重矩阵上加一个低秩扰动项） | 插入一个完整的MLP模块（非线性+残差） |
> | 是否训练原始参数   | 不训练                                          | 不训练                               |
> | 是否修改主模型输出 | 是，修改了输出                                  | 是，Adapter的残差路径会添加到主路径  |
> | 是否修改模型结构   | 否，只影响输出，不改变结构                      | 是，真正新增了模块和参数             |
> | 构造复杂度         | 只加线性低秩扰动，无需新增激活函数              | 新增模块+非线性                      |
> | 参数量             | 非常少（只训练$r<<d$的矩阵）                    | 也较少，但比LoRA多，且多了非线性操作 |
> | 部署合并/插入方式  | **可选择**合并到原有参数：$W^{'}=W+\Delta W$      | 不合并，是一个新的组件，只能保留外挂 |
> | 框架兼容性         | 极高（保持原模型结构）                          | 需要模型支持结构插入                 |


</details>


### Prompt-based 微调

不改模型、不改参数，仅通过**构造或学习输入prompt**让模型做特定任务。有以下方法

<details>
<summary>Prompt Tuning</summary>

<br>

**核心思想**

> **只训练一小段可学习的“虚拟提示词”向量（soft prompt）来引导大模型完成任务，保持模型主干参数完全不变。**

**数学形式表达**

> 以语言模型为例，原始输入是 token embedding：
> 
> $$
> Input=[e(x_1),e(x_2),...,e(x_n)]
> $$
> 
> Prompt Tuning 添加 m 个“可学习的伪 token embedding”作为前缀：
> 
> $$
> Input=[p_1,p_2,...p_m,e(x_1),e(x_2),...,e(x_n)]
> $$
> 
> 其中：
> - $p_i\in \mathbb{R}^d$：是可学习的向量（不是具体词），初始化可以为随机值或词向量
> - $e(x_i)$：是真实输入的 token embedding
> - $d$：embedding 维度，必须与模型一致

**训练流程**

> 1. 冻结模型所有参数
> 2. 初始化 soft prompt（比如 10 个虚拟 token embedding 向量），初始化策略：
>    1. 随机初始化（Gaussian）：常用，效果稳
>    2. 使用某些 token 的预训练 embedding
>    3. 使用任务相关词初始化（如 "classify", "question", "text"）
> 3. 构建模型输入：拼接 soft prompt + 原始输入 token embedding
> 4. 前向传播 → 损失计算 → 反向传播
> 5. 只更新 prompt embedding 参数

**优缺点**

> 优点：
> 1. 极致轻量：训练参数极少，几乎不占显存
> 2. 主模型不变：方便部署、版本控制、多人协作
> 3. 适配多任务：每个任务都可以训练一组 soft prompt，快速切换
> 
> 缺点：
> 1. 表达能力有限：尤其在复杂任务（如多轮对话、结构化 QA）中不如 Adapter/LoRA
> 2. 依赖语言模型的 prompt 理解能力
> 3. 对输入序列长度较敏感（插入 prompt 后输入可能变长）


**Prompt Tuning 与手写 Prompt（Hard Prompt）对比**

> |            | Hard Prompt        | Soft Prompt (Prompt Tuning)       |
> | ---------- | ------------------ | --------------------------------- |
> | 类型       | 文本字符串         | 训练得到的 embedding 向量         |
> | 是否可训练 | ❌ 不可             | ✅ 可训练                          |
> | 表达能力   | 受限于词表         | 连续空间更强大                    |
> | 训练目标   | 无需训练           | 任务驱动训练                      |
> | 例子       | "Classify: [text]" | learnable vector1 + vector2 + ... |

</details>


<details>
<summary>Prefix Tuning（前缀调控）</summary>

<br>

**核心思想**

> 仅在每一层 Transformer 的注意力机制中注入一段“可训练的前缀向量（prefix）”（注入引导信号），从而在不修改主模型参数的前提下引导模型完成新任务。

**原理与结构**

> 在每一层 attention 中，为 K 和 V 添加一个可训练的前缀向量：
> 
> $$
> K'=[P_K,K],V'=[P_V,V]
> $$
> 
> - $P_K$和$P_V$是 prefix tuning 中训练得到的前缀 key/value
> - 它们是任务相关的，且只训练它们，主模型参数全部冻结
> - 这些 prefix 向量通常是通过一个小 MLP（多层感知机）从可训练 embedding 投影得到

**实现流程（训练阶段）**

> 1. 初始化一个 prefix embedding matrix（shape：`[prefix_len, d_model]`）
> 2. 通过一个小 MLP（或线性投影）得到对应的$P_K$和$P_V$
> 3. 在每一层 attention 中插入 prefix，形成新的$K'$和$V'$
> 4. 模型其余参数全部冻结
> 5. 只训练 prefix embedding & 其投影网络（MLP）
> 
> 原本的：
> 
> $$
> Attention(Q,K,V)=softmax(\frac{QK_T}{\sqrt{d_k}})V
> $$
> 
> 其中：
> - $Q = X * W_Q$
> - $K = X * W_K$
> - $V = X * W_V$
> 
> 
> **现在的：**
> 
> $$
> Prefix\ Embedding → MLP → P_K, P_V
> $$
> 
> $$
> K' = [P_K, K],V' = [P_V, V]
> $$
> 
> $$
> Attention(Q,K',V')=...
> $$

**优缺点**

> | 优点             | 说明                             |
> | ---------------- | -------------------------------- |
> | 高效训练         | 只训练极少量 prefix 向量         |
> | 模型冻结         | 主模型参数完全不变，部署稳定     |
> | 多任务灵活       | 每个任务只需切换 prefix 文件     |
> | 可与其他技术组合 | 可结合 LoRA、Adapter、RAG 等使用 |
> 
> 
> | 缺点                     | 说明                                     |
> | ------------------------ | ---------------------------------------- |
> | 实现复杂                 | 需要修改 attention 输入结构              |
> | 推理略慢                 | 每层都要加 prefix，会增加计算量          |
> | **对分类任务性能略逊色** | 相比 LoRA、Adapter，**生成类任务更擅长** |

**为什么只在KV上加，不在Q上加**

> **Q是什么？为什么不加**
> 
> Q 是当前输入 token 在当前位置的投影结果。代表当前 token 的“注意力提问”。表示“我现在在这个位置想知道什么”
> 所以我们希望每个输入 token 的 Query 维持“原始的表示”，这样我们就可以观察这些 token 如何被上下文引导
> 如果我们去改 Q，就等于改变了 token 自己的语义表示，会干扰原始任务。
> 
> **KV是什么？为什么加**
> 
> K（Key） 和 V（Value）：代表上下文 token 的“信息与回答”
> 我们设计 prefix 的目标是：
> - 为每个原始 token **提供额外的任务相关的“上下文信息”**，让所有输入 token 都能 attend 到这段信息，而不是去改变 token 本身的表示或行为。
> - 这段 prefix 就像是“系统引导语”、“虚拟上下文”或者“专家先知”
> 
> 这正是 Key/Value 的语义作用：
> - K：告诉其他 token “我这是什么信息”
> - V：提供实际的信息内容
> 
> **总结就是：**
> Prefix Tuning 不修改 Q，是因为 Q 代表当前 token 自身的提问，我们希望它维持原始语义，只是让它能够 attend 到更多任务相关的提示（通过 K/V 前缀）。

**加入Q也引入，会发生什么？（有些研究确实也这么做过）**

> 我们原本的 Q 是“真实输入 token 对其他 token 发起的提问”
> 现在引入 prefix Q 相当于说：
> 还有一些“虚拟 token”（prefix query）也想要访问上下文信息
> 这些 prefix-Q 只是“发问者”，不参与最终输出（如果你不保留 prefix-Q 的输出）
> 
> 这种做法的**意义可能**包括：
> - 模拟系统提示的关注点：某些任务需要系统主动提问（如问答、摘要等）
> - 增强注意力分布的学习能力
> 
> **会有什么后果？**
> 
> 潜在优点：
> - 提升表达能力：Q/K/V 同步调控能更灵活地建模复杂任务
> - 强引导性：适用于需要“强先验提示”的任务（如代码生成、逻辑推理）
> - 保持原始参数冻结，仍然是轻量微调
> 
> 潜在问题：
> - 训练不稳定：Q 是对输入 token 编码的结果，如果改动太大会干扰模型语义建构过程
> - 维度对齐复杂：必须确保新引入的 Q、K、V 在 head 和 batch 维度完全一致
> - 难以泛化：如果 prefix-Q 太强，引导性过度，反而可能在新任务上过拟合
> 
> | 修改 Q？                   | 含义                                  | 影响                           |
> | -------------------------- | ------------------------------------- | ------------------------------ |
> | 不改（经典 Prefix Tuning） | 输入 token 自身语义不动，只引导注意力 | 稳定、泛化好                   |
> | 改 Q（如 P-Tuning v2）     | 引入虚拟提问者，引导注意力方向        | 更强表达力，训练复杂，易过拟合 |

</details>

<details>
<summary>P-Tuning（prompt embedding）</summary>

<br>

P-tuning（prompt embedding），和Prompt tuning有点像，这边简单介绍：

**P-Tuning v1**
> 同样在输入层加 embedding
> 区别在于它强调“使用 LSTM 学习 prompt 表示”（v1 版本）
> prompt 是通过 BiLSTM + MLP 得到的向量序列，而不是直接训练向量
> 这就让 prompt 更具有结构建模能力（顺序信息、组合表达），在结构化 NLP 任务中更强。

**P-Tuning v2**
> 改进 P-Tuning v1，去掉了 LSTM，直接训练 embedding 向量（像 Prompt Tuning）
> 但是在每一层 Transformer 中都注入 prompt 向量（像 Prefix Tuning）
> 这解决了 Prompt Tuning 在大模型下训练困难、收敛慢、效果差的问题。

**区别**

> | 方法          | 类比                                                             |
> | ------------- | ---------------------------------------------------------------- |
> | Prompt Tuning | 给模型塞进一些“伪词”，希望它自己理解这些词代表“你要做情感分类哦” |
> | P-Tuning v1   | 把伪词通过 LSTM 编码，让模型更清楚这些是“引导任务的逻辑序列”     |
> | P-Tuning v2   | 把伪词嵌入每一层 Transformer，使整个模型每层都知道你是来做分类的 |


</details>

# 如何选择微调方式？

| 条件/目标            | 推荐微调方法             |
| -------------------- | ------------------------ |
| 数据量大，资源足够   | 全参数微调               |
| 希望快速实验、部署   | LoRA                     |
| 多任务共享模型       | Adapter                  |
| 极限压缩参数         | Prompt/Prefix Tuning     |
| 不允许改动原模型参数 | PEFT 系列（尤其是 LoRA） |

