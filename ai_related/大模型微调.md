- [LLM 微调基本定义与流程](#llm-微调基本定义与流程)
- [LLM 常见微调方法](#llm-常见微调方法)
  - [全参数微调（Full Fine-tuning）](#全参数微调full-fine-tuning)
  - [参数高效微调方法（PEFT）](#参数高效微调方法peft)
    - [LoRA（Low-Rank Adaptation）](#loralow-rank-adaptation)
    - [Adapter 微调](#adapter-微调)


# LLM 微调基本定义与流程

<details>
<summary>LLM 微调基本定义与流程</summary>

<br>

**定义**

> 大模型的微调（fine-tuning）是指在预训练模型（如 GPT、BERT、LLaMA 等）基础上，**使用特定任务或领域的数据**对模型进行**再次训练**，以适应具体应用场景的需求。

**基本流程**

> 1. 选择预训练模型：选择一个经过大规模语料训练的模型，如 LLaMA、GPT-3、BERT、ChatGLM 等。
> 2. 准备数据集：数据应针对具体任务，数据格式需与模型输入兼容，通常是 JSON 格式。
>    1. 文本分类：标签化的文本数据
>    2. 问答系统：问答对
>    3. 对话系统：多轮对话
>    4. 信息抽取：带标注的文本等
> 3. 选择微调方法：
>    1. 全参数微调（Full Fine-tuning）
>    2. 参数高效微调（PEFT）
> 4. 训练设置：
>    1. 超参数设置（学习率、批大小、训练轮次）
>    2. 设备选择（GPU/TPU，分布式训练）
> 5. 评估与部署：
>    1. 在验证集测试效果
>    2. 保存模型权重用于部署

</details>

# LLM 常见微调方法

## 全参数微调（Full Fine-tuning）

**定义**

在保持原有大模型结构不变的情况下，对模型的**所有参数**（包括 Transformer 的 attention、MLP、LayerNorm 等）进行梯度更新训练，使其适应新的下游任务。

**标准微调（Vanilla Fine-tuning）**

1. 加载预训练模型
2. 准备下游任务数据（分类/生成/问答/翻译等）
3. 使用任务 loss（如交叉熵）训练整个模型

**分层微调（Layer-wise Fine-tuning）**

并非所有层一起训练，而是：

1. 从顶层（靠近输出）开始微调
2. 然后逐步解冻中间层、底层

**冻结部分参数（Partial Freezing）**

1. 冻结部分模块，如 embedding 层、低层 Transformer
2. 只训练中高层

**学习率调度（Layer-wise Learning Rate Decay, LLRD）**

目标：为模型中**不同层设置不同的学习率**，通常是底层（靠近输入）较小，顶层（靠近输出）较大。

这样做可以更稳定地训练大模型，尤其是如 BERT、RoBERTa、LLaMA 这种“层次结构明显”的 Transformer 模型。

<details>
<summary>为什么要用 LLRD？</summary>

<br>

> 大模型中各层的语义含义不同：
> 
> | 层级              | 表达内容                 | 微调建议                 |
> | ----------------- | ------------------------ | ------------------------ |
> | 底层（input 近）  | 通用特征（如词法、句法） | 保留原始能力（学习率小） |
> | 中间层            | 半通用语义               | 适度调整                 |
> | 顶层（output 近） | 任务相关特征（如分类）   | 重点微调（学习率大）     |

</details>


<details>
<summary>LLRD 的核心思想</summary>

<br>

> 给模型的每一层分配不同的学习率，通常按照指数衰减：
> 
> $$
> lr_l = base\_lr \times (decay\_rate)^{L-l-1}
> $$
> 
> 其中：
> - $l$：当前层编号（从底到顶）
> - $L$：总层数
> - $base\_lr$：顶层的初始学习率
> - $decay\_rate$：通常是 0.8 ~ 0.95
> 
> 使用示例：
> 
> | 情况                    | 建议                            |
> | ----------------------- | ------------------------------- |
> | 模型层数多（12 层以上） | 强烈建议使用 LLRD，微调更稳定   |
> | 训练数据量少            | LLRD 可避免模型过度拟合底层参数 |
> | 使用低学习率训练        | 配合 LLRD 提升梯度利用效率      |
> | 与全参数微调搭配        | 非常适合，互补                  |


</details>

## 参数高效微调方法（PEFT）

### LoRA（Low-Rank Adaptation）

<details>
<summary>LoRA（Low-Rank Adaptation）</summary>

<br>

**LoRA 的核心思想**

> 用低秩矩阵近似表示参数的微调变化，只训练这部分低秩矩阵，而不更新原始模型参数。
> 
> 以 Transformer 中的线性层为例：
> 
> - 原始的线性层为一个权重矩阵 $W_0\in{\mathbb{R}^{d\times{k}}}$，输入为 $x\in{\mathbb{R}^k}$，输出为 $y=W_0x$
> - 微调的目标是让 $W_0$ 稍作变化：$W=W_0+\Delta{W}$
> - 问题在于直接训练 $\Delta{W}\in{\mathbb{R}^{d\times{k}}}$ 仍然很大
> 
> LoRA 的做法：
> 
> - 用两个低秩矩阵 $A\in{\mathbb{R}^{d\times{r}}}$、$B\in{\mathbb{R}^{r\times{k}}}$ 近似表示 $\Delta{W}$，即 $\Delta \approx AB$
> - $r\ll \min (d,k)$，所以参数量大大减少，从 $dk$ 减少到 $r(d+k)$
> - 同时冻结原始参数 $W_0$，只训练 $A$ 和 $B$。

**数学形式表达**

> 设原始线性变换为：
> 
> $$
> y=W_0x
> $$
> 
> 引入 LoRA 后，新的线性变换为：
> 
> $$
> y=W_0x+\Delta Wx=W_0x+ABx
> $$
> 
> 其中：
> 
> - $W_0\in{\mathbb{R}^{d\times{k}}}$：冻结的原始权重
> - $A\in{\mathbb{R}^{d\times{r}}}$：可训练的矩阵
> - $B\in{\mathbb{R}^{r\times{k}}}$：可训练的矩阵
> - $r$ 是秩，决定了压缩程度，通常 $r=4,8,16$ 等
> - 由于 $rank(AB)\leq r$，这就构成了一个低秩近似
> 
> 有时还会加入缩放因子 $\alpha$，使输出更稳定：
> 
> $$
> y=W_0x+\frac{\alpha}{r}ABx
> $$

**什么是低秩矩阵？为什么两个低秩矩阵可以表示原来的矩阵？**

> 矩阵的秩可以理解为**矩阵中包含的有效信息维度的数量**，也就是线性无关的行（或列）的最大数量。
> 
> - 对于一个 $d\times k$ 的矩阵 $W$，如果它的秩是 $r$，说明它实际上只包含 $r$ 维的有效信息。
> - 通俗点说，它“看上去”是 $d\times k$，但其实信息都集中在一个低维空间里。
> 
> 任意一个秩为 $r$ 的矩阵 $\Delta{W}\in{\mathbb{R}^{d\times{k}}}$，都可以分解为两个矩阵的乘积：
> 
> $$
> W=AB,其中 A\in{\mathbb{R}^{d\times{r}}}, B\in{\mathbb{R}^{r\times{k}}}
> $$
> 
> 这是线性代数中著名的定理，例如 **SVD**（奇异值分解）就可以用来得到最优的低秩近似。
> 
> 在 LoRA 中，我们不是去精确分解原始 $W$，而是让更新量 $\Delta W$，**从一开始就限制在一个低秩空间**（因为训练参数就这么多），这样保证微调是轻量的。

**训练与推理阶段**

> 训练阶段：冻结原始模型权重 $W_0$，只训练 $A$ 和 $B$，参数量显著减少，训练更快、占用更少内存。
> 推理阶段：可以选择将 $W=W_0+AB$ **合并**成一个矩阵推理；或者保持**分离**结构计算 $W_0x+ABx$
> 
> |      | 合并推理                                                                           | 分离推理                                                                       |
> | ---- | ---------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
> | 优点 | 推理更快，只需一次矩阵乘法。                                                       | 模块化，可以动态加载不同的 $A$ 和 $B$ 比如做多任务、多语言时切换 LoRA 更方便。 |
> | 缺点 | 如果任务很多、需要动态加载不同任务的 LoRA 参数，就必须为每个任务都构建一个新的 $W$ | 比合并方式慢一点（额外一次乘法和加法），占用更多内存。                         |


**模型中哪些矩阵要用LoRA，怎么选**

> 选择使用LoRA的依据：
> - 对模型表现最关键的层（如 attention 中的 Q、V）
> - 参数量大的层（线性层更适合低秩分解）
> - 在不同任务中变化比较大的层（更值得微调）
> 
> 比如对于原来的 Query 投影会变成：
> 
> $$
> Q = W_q x + \frac{\alpha}{r} A_q B_q x
> $$
> 
> 其中的 $A_q$ 和 $B_q$ 是 LoRA 添加的低秩模块，只有这两个是可训练参数。

**LoRA与原始权重参数是怎么共存的**

> **LoRA 不会替换或修改原始模型参数，而是“旁路地添加一个可学习的低秩偏置项”，在前向传播时与原始输出相加，在反向传播时只更新低秩参数。**
> 
> **LoRA 的核心价值是“旁路添加”而不是“替换”：**
> 
> 1. 原始模型就像高速公路
> 2. LoRA 是旁边加了一条可调节的辅道
> 3. 推理时，原始高速公路照旧跑车，LoRA 的辅道可以开关（合并、移除都行）
> 
> | 问题               | 说明                             |
> | ------------------ | -------------------------------- |
> | 是否修改原始参数 W | ❌ 不改                           |
> | 是否添加新参数     | ✅ 添加 BA，秩很低                |
> | 是否可独立部署     | ✅ 原始模型和 LoRA 参数可分开保存 |
> | 推理时是否可合并   | ✅ 可合并为新 W                   |
> | 是否替代原矩阵     | ❌ 不是替换，而是“添加扰动”       |

**LoRA优缺点与使用建议**

> | 优点                       | 说明                                                                                                   |
> | -------------------------- | ------------------------------------------------------------------------------------------------------ |
> | 显著减少训练参数量         | 只训练两个小矩阵，相比于数亿全量参数，节省训练开销巨大 （<1%）                                         |
> | 省显存                     | 不更新原模型参数，避免了反向传播中巨大的梯度缓存，显著降低显存使用                                     |
> | 模块化设计，便于部署与共享 | LoRA 参数是外挂模块，不影响原始模型权重，可重复使用，可在多个任务间共享底座模型，只传/存小模型参数即可 |
> | 支持热插拔与推理融合       | 推理时可“合并 LoRA”到原参数中，部署灵活高效，或动态开启/关闭 LoRA 模块进行 A/B 测试                    |
> | 训练更快，资源利用率更高   | 少参数可更快收敛；多个任务可并行训练多个 LoRA，不需要复制整个模型                                      |
> | 适合多任务微调或少样本学习 | 多个 LoRA 对应多个任务（如对话、摘要、情感分析），只需载入对应 LoRA 权重即可切换任务                   |
> 
> | 缺点                                   | 说明                                                                                                       |
> | -------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
> | 性能可能不如全参微调（尤其数据量足时） | LoRA 的表达能力受限于秩 r，如果任务复杂、训练数据丰富，LoRA 的最终表现略逊全参微调（特别是在领域外迁移时） |
> | 不适合所有模块                         | LoRA 通常插入到 Attention 的 Q、V 或 Linear 层，对于非 Transformer 架构或 CNN、RNN 等模型适配性差          |
> | 需要调试超参数（rank、alpha）          | 不同任务对 LoRA 的秩（r）和缩放因子（α）敏感，需要试验调整，缺乏通用默认值                                 |
> | 引入结构复杂性                         | 多个 LoRA 插件（多任务场景）管理复杂，部署流程需要适配 LoRA 合并或动态载入，可能增加工程复杂度             |
> | 对低层特征学习能力有限                 | 由于只在高层添加微调能力，如果任务强依赖于低层感知（如多模态、图像分割），LoRA 表现可能受限                |
> 
> | 场景                   | 是否推荐使用 LoRA | 理由                                  |
> | ---------------------- | ----------------- | ------------------------------------- |
> | 文本分类 / NLU 任务    | ✅ 推荐            | 高层语义表达，LoRA 表现良好           |
> | 文本生成 / 翻译任务    | ✅ 推荐            | 模型容量大时，LoRA 具备很强的效率优势 |
> | 小样本或私有数据微调   | ✅ 强烈推荐        | 快速上手，成本低，适合少样本          |
> | 多任务共享底座模型     | ✅ 非常适合        | 每个任务一组 LoRA 权重即可            |
> | 图像任务或低层特征重要 | ❌ 不推荐          | 不适合结构完全不同的模型              |
> | 大量标注数据且资源充足 | ✅ 全参微调优先    | 性能上限更高                          |


</details>


### Adapter 微调

<details>
<summary>Adapter 微调</summary>

<br>

**核心思想**

> 在预训练模型的每一层或若干层之间**插入一个小型的“Adapter 模块”**，并且只训练这些模块的参数，其它原始模型参数保持冻结不变。
> 
> 这边是加了一个**模块**，而LoRA则是加了个 $\Delta W$，这是很大的区别。

**Adapter 模块结构**

> Adapter 模块一般是一个小型 MLP，采用瓶颈结构：
> 
> $$
> Adapter(x)=x+W_{up}\times ReLU(W_{down}x)
> $$
> 
> 其中：
> - $W_{down}\in \mathbb{R}^{d\times r}$：降维矩阵（r 是 bottleneck 维度，如 8、16）
> - $W_{up}\in \mathbb{R}^{r\times d}$：升维矩阵
> - 输入/输出维度不变，因此可以与主干模型无缝集成
> - 使用残差连接（加法），避免破坏原始特征

**Adapter 优缺点**

> | 优点             | 说明                                                                    |
> | ---------------- | ----------------------------------------------------------------------- |
> | 极大减少训练参数 | 只需训练 Adapter，小模型存储部署方便                                    |
> | 保留原始知识     | 主干模型保持冻结，不破坏已有的预训练知识                                |
> | 适用于多任务训练 | 每个任务独立训练 Adapter，主干模型共享（避免灾难性遗忘）                |
> | 动态插拔         | 推理时可插入/卸载 Adapter，灵活部署                                     |
> | 易于集成现有框架 | HuggingFace Transformers 已广泛支持 Adapter 加载与微调（如 AdapterHub） |
> 
> | 缺点               | 说明                                                          |
> | ------------------ | ------------------------------------------------------------- |
> | 表达能力受限       | Adapter 参数少，对复杂任务拟合能力不如全参数微调              |
> | 插入位置敏感       | 插入在 Attention 后 or FFN 后，位置选择影响性能，需要实验调整 |
> | 超参数需调试       | bottleneck 尺寸、激活函数、残差连接方式等均需调优             |
> | 增加训练代码复杂性 | 不如普通微调“直接训练模型”那么直观                            |

**Adapter与LoRA对比！！！**

> | 对比项             | LoRA                                            | Adapter                              |
> | ------------------ | ----------------------------------------------- | ------------------------------------ |
> | 训练的对象         | $\Delta W$ （在已有权重矩阵上加一个低秩扰动项） | 插入一个完整的MLP模块（非线性+残差） |
> | 是否训练原始参数   | 不训练                                          | 不训练                               |
> | 是否修改主模型输出 | 是，修改了输出                                  | 是，Adapter的残差路径会添加到主路径  |
> | 是否修改模型结构   | 否，只影响输出，不改变结构                      | 是，真正新增了模块和参数             |
> | 构造复杂度         | 只加线性低秩扰动，无需新增激活函数              | 新增模块+非线性                      |
> | 参数量             | 非常少（只训练$r<<d$的矩阵）                    | 也较少，但比LoRA多，且多了非线性操作 |
> | 部署合并/插入方式  | **可选择**合并到原有参数：$W^{'}=W+\Delta W$      | 不合并，是一个新的组件，只能保留外挂 |
> | 框架兼容性         | 极高（保持原模型结构）                          | 需要模型支持结构插入                 |


</details>



<!-- (3) Prefix Tuning / Prompt Tuning
原理：在模型输入前添加可学习的“前缀”（tokens 或 embedding）

Prefix-Tuning：

训练可学习的“前缀向量”，不更改主模型参数

Prompt Tuning：

在原始输入前添加一些可训练的 prompt tokens

优点：

更轻量级（只训练少量向量）

缺点：

表现略逊于全参数微调和 LoRA

需要精心设计 prompt embedding 的位置和数量

## 主流框架支持
| 框架                          | 支持的微调方式                   | 特点                |
| ----------------------------- | -------------------------------- | ------------------- |
| Hugging Face Transformers     | Full、LoRA、Adapter、Prefix      | 社区活跃、文档完备  |
| PEFT（from Hugging Face）     | LoRA、Prompt、IA³、Prefix        | 专注高效微调        |
| LLaMA-Factory / FastChat      | 支持 LLaMA、ChatGLM 的 LoRA 微调 | 支持对话模型        |
| OpenDelta                     | 支持 Adapter、LoRA、Prefix 等    | 支持各种 Delta 模型 |
| ColossalAI / DeepSpeed / FSDP | 高性能训练优化                   | 多用于全参数微调    |

## 实战场景举例
示例：使用 Hugging Face 对 LLaMA 模型进行 LoRA 微调
```python
from peft import get_peft_model, LoraConfig, TaskType
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```
之后使用 Trainer 训练即可。

## 如何选择微调方式？

| 条件/目标            | 推荐微调方法             |
| -------------------- | ------------------------ |
| 数据量大，资源足够   | 全参数微调               |
| 希望快速实验、部署   | LoRA                     |
| 多任务共享模型       | Adapter                  |
| 极限压缩参数         | Prompt/Prefix Tuning     |
| 不允许改动原模型参数 | PEFT 系列（尤其是 LoRA） |

## 微调 vs RAG vs Prompt Engineering

| 技术                | 是否训练  | 参数量 | 适用场景                 |
| ------------------- | --------- | ------ | ------------------------ |
| 微调                | ✅         | 中/大  | 精准任务适配             |
| RAG（检索增强生成） | ❌（可选） | 小     | 文档问答、知识密集型任务 |
| Prompt Engineering  | ❌         | 无     | 零样本、快速构建         | -->
