
- [RLHF整体流程](#rlhf整体流程)
- [第一步SFT监督微调](#第一步sft监督微调)


# RLHF整体流程

RLHF（Reinforcement Learning with Human Feedback，人类反馈强化学习）

> **整体流程**
> 
> 1. 阶段一：监督微调（SFT），用人类写好的问答对训练模型
> 2. 阶段二：奖励模型（RM）训练，人类对多个答案进行排序，然后训练奖励模型
> 3. 阶段三：强化学习微调，使用PPO等强化学习算法优化语言模型，用奖励模型给出奖励信号
> 
> **阶段 1：监督微调（Supervised Fine-Tuning, SFT）**
> 
> | 内容 | 说明                                               |
> | ---- | -------------------------------------------------- |
> | 数据 | 人类专家写好的问答对                               |
> | 目标 | 让预训练模型更符合“人类语言风格”和“高质量问答习惯” |
> | 方法 | 类似于常规的监督学习（Cross Entropy Loss）         |
> | 输出 | 初步具有人类风格的语言模型                         |
> | 优点 | 快速提升质量                                       |
> | 缺点 | 人工标注成本高，样本覆盖有限                       |
> 
> **阶段 2：奖励模型训练（Reward Model Training）**
> 
> | 内容         | 说明                                                                    |
> | ------------ | ----------------------------------------------------------------------- |
> | 数据         | 给定一个 prompt，模型生成多个候选回答，人类标注员排序（例如 A > B > C） |
> | 目标         | 训练一个奖励模型（RM）预测人类排序偏好                                  |
> | 方法         | Pairwise ranking loss（比如使用 Bradley-Terry loss）                    |
> | **模型结构** | 一般是 GPT 模型尾部接一个线性奖励头 r(x)，预测一个标量得分              |
> | 优点         | 模型学会区分“更人类友好”的输出                                          |
> | 缺点         | 排序主观性高，人力密集                                                  |
> 
> **阶段 3：使用 PPO 进行强化学习（PPO Fine-Tuning）**
> 
> | 内容     | 说明                                                |
> | -------- | --------------------------------------------------- |
> | 算法     | 使用强化学习算法（如 PPO）对语言模型进行微调        |
> | **状态** | prompt（提示词）                                    |
> | **动作** | 生成的文本                                          |
> | 奖励     | 奖励模型给出的得分 r(x)                             |
> | 优化目标 | 最大化期望奖励，同时不偏离原始模型太远（KL 正则项） |
> | 优点     | 模型逐步朝着人类偏好优化                            |
> | 缺点     | 训练不稳定、计算资源消耗大                          |
> 
> （为什么状态跟动作是prompt跟生成的文本，下面会解释）

# 第一步SFT监督微调

> **问题解答一：输入输出的方式**
> 
> LLM 本质上是一个自回归模型，也就是：**预测下一个 token 的概率，条件是前面的 token 序列**。
> 
> 因此，LLM 的输入输出不是 “固定长度” 的，而是：
> 
> > 输入：一个 token 序列
> > 输出：每个 token 的**下一个 token 概率分布**
> 
> 例如有一个训练样本：
> 
> > Human: 你是谁？
> > Assistant: 我是AI模型。
> 
> 我们把这整段拼成一个连续的文本：
> 
> > [Human: 你是谁？\nAssistant: 我是AI模型。]
> 
> 然后做一个很自然的**监督目标**：让模型预测每个 token 的下一个 token，最大化整个序列的似然概率（也就是最小化交叉熵损失）
> 
> 所以，SFT 就像是 “提示+答案”的拼接训练：
> - 把整段拼接成模型的输入；
> - 然后监督模型预测后面的部分。
> 
> 也就是说，prompt 只是输入上下文，真正要预测的是 response。
> 
> 那这样的输入模型岂不是看到“标签”了？
> 
> - 不会的。虽然拼接了，但训练只会监督模型从 prompt 开始，逐 token 去生成 response 部分。
> - **通过遮蔽机制避免了模型“提前看见答案”**。
> 
> **问题解答二：损失的计算方式**
> 
> - 模型会逐 token 地预测下一个 token 的概率。
> - 交叉熵损失只在 response 部分的 token 上进行累计计算。
> - prompt 的 token 仅作为 context 输入，不参与损失计算。
> 
> **问题解答三：交叉熵怎么用在这里？**
> 
> 举个简单的例子：
> - 假设当前预测的位置是"我"，标签是"是"。
> - 模型输出了一个 token 概率分布，比如：`"是": 0.2, "的": 0.1, "谁": 0.05, "AI": 0.01, ...`
> - 真正的标签是 "是"，所以交叉熵计算的是 -log(0.2)。
> - 如果模型把 "的" 的概率设为 0.8，那就错了，交叉熵会很大；
> - 反过来，如果把 "是" 的概率设为 0.9，那交叉熵很小，模型就得分高。
> 
> **模型会给出整个词表token的概率，然后来跟标签token做损失。**
> 
> **问题解答四：模型内部怎么用词表？**
> 
> 1. 输入文本 -> tokenizer -> token ids："你好" -> [1543, 3210]
> 2. token ids -> embedding 向量：embedding table是一个`[vocab_size x hidden_dim]` 的矩阵
> 3. 预测阶段：模型输出logits（得分）是`[batch_size, vocab_size]`，用 softmax 转成概率分布
> 4. 输出 token id -> tokenizer -> 文本
>
> 正是在输出层，每一步都需对整个词表计算 softmax，这导致计算瓶颈
>
> **示例**
>
> 对比图像任务：
> > 图像输入: 一张猫的图片；标签: "猫" 类别（class_id=3）
> > 预测: 模型输出 [0.1, 0.7, 0.05, 0.15] -> 最大值对应类别为“狗”
> > 损失: CrossEntropy(pred, label)
>
> 而NLP中的SFT：
> > 输入: 整段 token 序列（含 prompt 和回复），如“请介绍一下机器学习。机器学习是一种人工智能技术，它让计算机可以从数据中学习...”
> > 目标: 让模型预测每个 token 的下一个 token（一般从回复开始）
> > 损失: 对 token[i+1] 做交叉熵损失
> 
> | 项目           | 图像任务                     | NLP 的 LLM SFT                   |
> | -------------- | ---------------------------- | -------------------------------- |
> | 标签形式       | 单个 label（类 ID）或 vector | 一段完整的回复文本（token 序列） |
> | 输入是否含标签 | ❌ 标签是独立的               | ✅ 标签作为序列拼接在输入里       |
> | 损失位置       | 直接在输出向量上             | 在预测序列 token 上              |
> | 是否自回归     | 否                           | 是（每个 token 预测下一个）      |
