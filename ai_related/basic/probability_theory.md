- [概率的基本概念](#概率的基本概念)
- [一些基本公式](#一些基本公式)
  - [条件概率](#条件概率)
  - [全概率公式](#全概率公式)
  - [贝叶斯公式](#贝叶斯公式)
  - [KL散度（Kullback-Leibler Divergence）](#kl散度kullback-leibler-divergence)
- [最大似然估计、最大后验估计、贝叶斯](#最大似然估计最大后验估计贝叶斯)
  - [概率函数和似然函数的区别](#概率函数和似然函数的区别)
  - [最大似然估计](#最大似然估计)
  - [最大后验估计](#最大后验估计)
    - [L1、L2正则化的统计学意义](#l1l2正则化的统计学意义)


[参考链接](https://zhuanlan.zhihu.com/p/506449599)

# 概率的基本概念

**定义**

概率（Probability） 描述的是一个事件发生的可能性大小，是 0 到 1 之间的数：
- 0 表示“不可能发生”
- 1 表示“必然发生”
- 0.5 表示“一半的机会”

**基本术语**

| 概念                                   | 解释                           |
| -------------------------------------- | ------------------------------ |
| 样本空间（Sample Space, 记作$\Omega$） | 所有可能结果组成的集合         |
| 样本点（Sample Point）                 | 样本空间中的一个元素           |
| 事件（Event）                          | 样本空间的一个子集，即一组结果 |
| 基本事件                               | 只包含一个样本点的事件         |

示例：掷一颗六面骰子
- 样本空间 $\Omega$ = {1, 2, 3, 4, 5, 6}
- 一个事件 A = “掷出偶数” = {2, 4, 6}
- 一个基本事件 = {5}

**三大概率体系**

| 名称       | 解释                                         | 应用场景                         |
| ---------- | -------------------------------------------- | -------------------------------- |
| 古典概率   | 所有情况等可能，概率 = 有利结果数 / 总结果数 | 掷骰子、抽扑克牌                 |
| 频率概率   | 大量实验后，事件发生的频率趋于一个值         | 蒙特卡洛方法、统计实验           |
| 贝叶斯概率 | 把概率看作“主观信念”的程度                   | 深度学习中的贝叶斯推断、先验知识 |

**概率的三大公理**

- 公理一：非负性：任何事件的概率不能是负的。$P(A)\ge0$
- 公理二：规范性：整个样本空间的概率是 1。$P(\Omega)=1$
- 公理三：可加性：如果两个事件互不重叠（互斥），它们的联合概率是各自概率之和。$A\cap B=\varnothing \Rightarrow P(A\cup B)=P(A)+P(B)$

**概率与统计的区别**

概率（probabilty）和统计（statistics）看似两个相近的概念，其实研究的问题刚好相反。

- 概率：概率研究的问题是，已知一个模型和参数，怎么去预测这个模型产生的结果
- 统计：统计研究的问题则相反。统计是，有一堆数据，要利用这堆数据去预测模型和参数。
- 总结：**概率是已知模型和参数，推数据。统计是已知数据，推模型和参数**。


# 一些基本公式

## 条件概率

**定义：**

在事件 $B$ 已经发生的条件下，事件 $A$ 发生的概率。记作 $P(A|B)$，读作“在 $B$ 发生的条件下 $A$ 的概率”。

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

**性质：**
- $P(A|B)$ 反映了 $A$ 在 $B$ 已知发生时的概率。
- 若 $A$ 与 $B$ 独立，则 $P(A|B) = P(A)$。

**常见应用：**
- 用于推断在已知部分信息下其他事件发生的概率。
- 机器学习中的朴素贝叶斯分类器。

## 全概率公式

**定义：**

当一个复杂事件可以被拆成几个互斥子事件时，使用全概率公式：

$$
P(A) = P(A|B)P(B) + P(A|\bar{B})P(\bar{B})
$$

更一般地，若 $B_1, B_2, ..., B_n$ 构成一个完备事件组：
$$
P(A) = \sum_{i=1}^n P(A|B_i)P(B_i)
$$

**性质：**
- 适用于事件空间被划分为互斥且完备的子事件时。

**常见应用：**
- 复杂概率计算的分解。
- 贝叶斯推断中的分母展开。

## 贝叶斯公式

**定义：**

在观察到 $A$ 的前提下，推断 $B$ 的概率：

$$
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
$$

将分母展开得：

$$
P(B|A) = \frac{P(A|B)P(B)}{P(A|B)P(B) + P(A|\bar{B})P(\bar{B})}
$$

**性质：**
- 贝叶斯公式实现了先验概率到后验概率的更新。
- 需要已知条件概率和先验概率。

**常见应用：**
- 机器学习中的贝叶斯推断。
- 医学检测、风险评估等领域的概率反推。

## KL散度（Kullback-Leibler Divergence）

KL散度（又称相对熵）用来衡量两个概率分布 $P$ 和 $Q$ 之间的差异。它本质上描述了**用分布Q近似分布P**时，所“浪费”的信息量。

**定义：**

对于离散分布，KL散度定义为：
$$
D_{KL}(P\|Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$

对于连续分布，定义为：
$$
D_{KL}(P\|Q) = \int P(x) \log \frac{P(x)}{Q(x)} dx
$$

**性质：**
- $D_{KL}(P\|Q) \geq 0$，且当且仅当 $P=Q$ 时取等号（即KL散度最小为0）。
- KL散度不是对称的，即 $D_{KL}(P\|Q) \neq D_{KL}(Q\|P)$，所以它不是严格意义上的“距离”。

**常见应用：**
- 机器学习中用于衡量模型分布与真实分布的差异（如GAN、VAE等）。
- 信息论中衡量编码效率损失。
- 作为损失函数（如交叉熵损失本质上包含KL散度）。

# 最大似然估计、最大后验估计、贝叶斯

## 概率函数和似然函数的区别

似然（likelihood）这个词其实和概率（probability）是差不多的意思。

但是在统计里面，似然函数和概率函数却是两个不同的概念（其实也很相近就是了）。

对于函数$P(x|\theta)$，输入有两个：

- $x$：某一个具体的数据
- $\theta$：模型的参数

**概率函数**：如果$\theta$是已知确定的，$x$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点$x$，其出现概率是多少。（相当于机器/深度学习模型的**测试过程**，此时参数已训练好，$\theta$是确定的）

**似然函数**：如果$x$是已知确定的，$\theta$是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现$x$这个样本点的概率是多少。（相当于机器/深度学习模型的**训练过程**，此时参数还未确定，$x$是确定的）


## 最大似然估计

例如，投10次特殊的硬币（给定模型），出现7次正面3次反面（请注意，这里10次结果有顺序）（收集数据），现在要估计投这枚硬币出现正面的概率（求参数）。

解答：这是一个统计问题，回想一下，解决统计问题需要什么？ 数据！

此时$x$是已知的，参数$\theta$是未知的，它描述对于不同的模型参数，出现x这个样本点的概率是多少

出现实验结果$x$（即反正正正正反正正正反）的似然函数是可表示成：

$$
\begin{align*}
    f(x,\theta)&=(1-\theta)\times \theta \times \theta \times \theta \times \theta \times(1-\theta) \times \theta \times \theta \times \theta \\
    &=\theta^7 (1-\theta)^3
\end{align*}
$$

最大似然估计，顾名思义，就是要最大化这个函数，说得更简单点，就是求它的极大值点

$$
\mathop{\arg\max}\limits_{\theta}f(\theta)
$$

**(1) 取对数**

$$
\ln (f(X,\theta))=\ln (\theta^7(1-\theta)^3)=7\ln \theta + 3\ln (1-\theta)
$$

**(2) 求导**

$$
\ln^{'}(f(X,\theta))=\frac{7}{\theta}-\frac{3}{1-\theta}
$$

**(3) 令导数为0求解**

$$
7(1-\theta)-3\theta=0 \Rightarrow \theta=0.7
$$

这样，我们已经完成了对$\theta$的最大似然估计。即抛10次硬币，发现7次硬币正面向上，最大似然估计认为正面向上的概率是0.7。是不是非常直接，非常简单粗暴？没错，就是这样，谁大像谁！

如果未知参数有多个，则需要用取对数的似然函数对每个参数进行求偏导，使得所有偏导均为0的值，即为该函数的极值点，一般也是其最大似然估计值。

那为什么不是0.5呢？**我们所谓的正常硬币向上的概率为0.5，其实就是贝叶斯里的先验概率**。

## 最大后验估计

前面的最大似然估计是：求参数$\theta$，使似然函数$P(x|\theta)$最大。

最大后验估计（**贝叶斯估计的一种特例**）：求参数$\theta$，使似然函数$P(x|\theta)P(\theta)$最大。求得的$\theta$不单单让似然函数大，自己出现的先验概率$P(\theta)$也得大。

（**这有点像正则化里加惩罚项的思想，不过正则化里是利用加法，而MAP里是利用乘法**）

最大后验估计实际上是在最大化：

$$
P(\theta|x)=\frac{P(x|\theta)P(\theta)}{P(x)}
$$

不过因为$x$是确定的（即投出的“反正正正正反正正正反”），$P(x)$是一个已知值，所以去掉了分母$P(x)$（总之，这是一个可以由数据集得到的值）

---

举个例子来说明贝叶斯定理：

假设你去医院，医生想判断你是不是得了某种病。
- 事件 A：你得了这种病
- 事件 B：你做了个检测，检测结果是阳性

医生知道：

- 这种病在一般人群中的发病率是很低的，比如只有1%（这就是先验概率$P(A)$）
- 这个检测对病人准确率很高，比如检测病人阳性的概率是99%（检测的“灵敏度”，也就是似然$P(B|A)$）
- 但是检测有一定假阳性率，比如健康人检测也会有5%概率显示阳性（假阳性率，即$P(B|\bar{A})$）

问题：
你检测阳性了，你到底有多大概率是真的病人？


**贝叶斯定理**告诉我们：

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}=\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\bar{A})P(\bar{A})}
$$

其中：
- $P(A|B)$：检测阳性后你真正患病的概率（后验）
- $P(B|A)$：病人检测阳性的概率（似然）
- $P(A)$：你本来患病的概率（先验）
- $P(B)$：检测阳性的总概率

代入数字计算：
- $P(A)=1\%=0.01$
- $P(B|A)=99\%=0.99$
- $P(B|\bar{A})=5\%=0.05$
- $P(\bar{A})=99\%=0.99$

先算检测阳性的总概率：

$$
P(B)=P(B|A)P(A)+P(B|\bar{A})P(\bar{A})=0.99\times 0.01+0.05\times 0.99=0.0594
$$

最后算后验：

$$
P(A|B)=\frac{0.99\times 0.01}{0.0594}\approx0.1667=16.67\%
$$

结论：虽然检测阳性，但你实际上只有大约 16.7% 的概率真的患病。这主要是因为病本来很少见，假阳性也不少。

### L1、L2正则化的统计学意义

在贝叶斯里，我们关心的是参数$\theta$的后验分布：

$$
P(\theta|D)\propto P(D|\theta) · P(\theta)
$$

最大后验估计（MAP）是说：找一个让上面这个后验概率最大的$\theta$，也就是：

$$
\hat{\theta}_{MAP}=\mathop{\arg\max}\limits_{\theta} (\log P(D|\theta) + \log P(\theta))
$$

**注意形式是不是很眼熟？第一项是损失函数，第二项是正则项！**

**关键联系：正则项 = 先验分布**


**L1正则化**

L1 正则项是参数的绝对值和：

$$
\lambda \sum\limits_i |\theta_i|
$$

对应对参数$\theta$加入一个 零均值的拉普拉斯分布先验：

$$
p(\theta)=\prod\limits_i Laplace(\theta_i|0,b) \propto\exp(-\frac{1}{b}\sum\limits_i |\theta_i|)
$$

概率理解：
- L1 正则化表示你相信大部分参数应该为 0，只有少数是非零的。
- 拉普拉斯分布的尖峰和重尾特性会导致稀疏解（有些参数被压到严格为 0）。
- 更强烈地推动“简洁模型”，有助于特征选择。

**L2正则化**

L2 正则项是参数的平方和：

$$
\lambda\sum\limits_i \theta_i^2
$$

等价于对参数$\theta$引入一个零均值的高斯先验：

$$
p(\theta)=\prod\limits_i \mathcal{N}(\theta_i|0,\sigma^2) \propto\exp(-\frac{1}{\sigma^2}\sum\limits_i \theta_i^2)
$$

概率理解：
- L2 正则化表示你相信参数应该服从正态分布，集中在 0 附近但允许较大值的出现。
- 惩罚大参数，但不会使参数严格为 0。
- 平滑、小幅惩罚，鼓励整体较小但非稀疏的参数。