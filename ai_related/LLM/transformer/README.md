

1. tokenizer 跟 embedding 是什么意思，怎么做的
2. 为什么需要位置编码，位置编码方法的发展，优缺点
3. QKV 的直观理解
4. 计算 QK 相似度的时候为什么用点积，可不可以用欧式距离或者余弦相似度，为什么
5. 为什么 QKV 的时候要除以根号dk，softmax公式是什么？softmax 遇到很大的值怎么办，指数直接导致数值溢出，现在的 softmax 都是怎么做的
6. 注意力机制，现在有哪几种主流的注意力改进
7. 为什么用多头注意力，有什么好处
8. 原始 transformer 中为什么有两个 attention 不需要 mask，有一个需要 mask，mask是怎么做的？
9. 前馈层的原理是什么，有什么作用
10. layer normalization 原理是什么，跟 batch normalization 的区别


